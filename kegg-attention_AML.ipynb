{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094868a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ed8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "['ACSS1', 'ACSS2', 'ADH1A', 'ADH1B', 'ADH1C', 'ADH4', 'ADH5', 'ADH6', 'ADH7', 'ADPGK', 'AKR1A1', 'ALDH1B1', 'ALDH2', 'ALDH3A1', 'ALDH3A2', 'ALDH3B1', 'ALDH3B2', 'ALDH7A1', 'ALDH9A1', 'ALDOA', 'ALDOB', 'ALDOC', 'BPGM', 'DLAT', 'DLD', 'ENO1', 'ENO2', 'ENO3', 'ENO4', 'FBP1', 'FBP2', 'G6PC1', 'G6PC2', 'G6PC3', 'GALM', 'GAPDH', 'GAPDHS', 'GCK', 'GPI', 'HK1', 'HK2', 'HK3', 'HKDC1', 'LDHA', 'LDHAL6A', 'LDHAL6B', 'LDHB', 'LDHC', 'MINPP1', 'PCK1', 'PCK2', 'PDHA1', 'PDHA2', 'PDHB', 'PFKL', 'PFKM', 'PFKP', 'PGAM1', 'PGAM2', 'PGAM4', 'PGK1', 'PGK2', 'PGM1', 'PGM2', 'PKLR', 'PKM', 'TPI1']\n"
     ]
    }
   ],
   "source": [
    "# genes-pathways annotation\n",
    "# hsa00010是指某一个通路\n",
    "# DNA\n",
    "path = './KEGG_pathways/20230205_kegg_hsa.gmt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "print(len(files))\n",
    "paways_genes_dict = {}\n",
    "for i in files: \n",
    "    paways_genes_dict[i.split('\\t')[0].split('_')[0]] = i.replace('\\n','').split('\\t')[2:] \n",
    "print(paways_genes_dict['hsa00010'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376544fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsa00010\n",
      "['hsa-mir-20b', 'hsa-mir-214', 'hsa-mir-545', 'hsa-mir-1285', 'hsa-mir-106b', 'hsa-mir-489', 'hsa-mir-192', 'hsa-mir-939', 'kshv-miR-K12-5*', 'hsa-let-7c', 'hsa-mir-587', 'hsa-mir-518a', 'hsa-mir-181c', 'hsa-mir-135a', 'hsa-mir-802', 'hsa-mir-484', 'hsa-mir-136', 'hsa-mir-424', 'hsa-mir-92b', 'hsa-mir-15b', 'hsa-mir-576', 'hsa-mir-605', 'hsa-mir-1304', 'hsa-mir-4286', 'hsa-mir-3928', 'hsa-mir-650', 'hsa-mir-122', 'hsa-let-7d', 'hsa-mir-103a', 'hsa-mir-765', 'hsa-mir-30e', 'hsa-mir-3613', 'hsa-mir-145', 'hsa-mir-361', 'hsa-mir-5006', 'hsa-mir-526b', 'kshv-miR-K12-7', 'hsa-mir-92a', 'hsa-mir-30d', 'hsa-mir-328', 'hsa-mir-30b', 'hsa-mir-138', 'hsa-mir-383', 'hsa-mir-378g', 'hsa-mir-615', 'hsa-mir-151a', 'hsa-mir-125b', 'hsa-mir-99b', 'hsa-mir-139', 'hsa-mir-326', 'hsa-mir-128', 'hsa-mir-548i', 'hsa-mir-221', 'hsa-mir-940', 'hsa-mir-3187', 'ebv-miR-BART5*', 'hsa-mir-548w', 'hsa-mir-493', 'hsa-mir-10b', 'hsa-mir-15a', 'hsa-mir-34b', 'hsa-mir-155', 'hsa-mir-1254', 'kshv-miR-K12-2*', 'hsa-mir-1295a', 'hsa-mir-3065', 'hsa-mir-186', 'hsa-mir-372', 'hsa-mir-16', 'hsa-mir-1301', 'hsa-mir-520c', 'hsa-mir-652', 'hsa-mir-27a', 'hsa-mir-3177', 'hsa-mir-29b', 'hsa-mir-520h', 'hsa-mir-365b', 'kshv-miR-K12-10a', 'hsa-mir-129-2', 'hsa-mir-30c', 'kshv-miR-K12-3', 'hsa-mir-199a', 'hsa-mir-486', 'hsa-mir-629', 'hsa-mir-34a', 'hsa-mir-369', 'hsa-mir-3605', 'hsa-mir-142', 'kshv-miR-K12-9*', 'hsa-mir-876', 'kshv-miR-K12-2', 'hsa-mir-379', 'hsa-mir-766', 'hsa-mir-150', 'hsa-mir-455', 'hsa-mir-874', 'hsa-mir-224', 'hsa-mir-3194', 'ebv-miR-BART20', 'hsa-mir-1290', 'hsa-mir-196a', 'hsa-mir-146a', 'hsa-mir-30c-2', 'hsa-mir-30a', 'hsa-mir-215', 'hsa-mir-4461', 'hsa-mir-527', 'hsa-mir-1270', 'hsa-mir-2964a', 'hsa-mir-579', 'ebv-miR-BART21', 'hsa-mir-24', 'hsa-mir-5690', 'hsa-mir-9', 'hsa-mir-324', 'hsa-mir-375', 'hsa-mir-574', 'hsa-mir-212', 'hsa-mir-135b', 'hsa-mir-19a', 'hsa-mir-873', 'hsa-mir-320d', 'hsa-mir-124', 'hsa-mir-421', 'kshv-miR-K12-5', 'hsa-mir-340', 'hsa-mir-502', 'hsa-mir-4521', 'kshv-miR-K12-11*', 'hsa-mir-365a', 'hsa-mir-183', 'hsa-mir-3150b', 'hsa-mir-657', 'hsa-mir-1185', 'hsa-let-7e', 'hsa-mir-500b', 'hsa-mir-3611', 'hsa-mir-548n', 'hsa-mir-412', 'hsa-mir-296', 'hsa-mir-499a', 'hsa-mir-130b', 'hsa-mir-19b', 'hsa-mir-584', 'hsa-mir-3940', 'hsa-let-7i', 'hsa-mir-152', 'hsa-mir-643', 'hsa-mir-1299', 'ebv-miR-BART16', 'hsa-mir-101', 'hsa-mir-140', 'hsa-mir-126', 'hsa-mir-941', 'hsa-mir-193a', 'hsa-mir-196b', 'hsa-mir-589', 'ebv-miR-BART1', 'hsa-mir-769', 'hsa-mir-497', 'hsa-mir-5682', 'hsa-mir-541', 'hsa-mir-4328', 'hsa-mir-513a', 'hsa-mir-194', 'hsa-mir-92a-1', 'hsa-mir-449c', 'hsa-mir-561', 'hsa-mir-1305', 'hsa-mir-431', 'hsa-mir-143', 'hsa-let-7g', 'hsa-mir-2682', 'hsa-mir-146b', 'hsa-mir-34c', 'hsa-mir-590', 'hsa-mir-744', 'hsa-mir-4791', 'hsa-mir-628', 'hsa-mir-4685', 'hsa-mir-381', 'hsa-mir-342', 'hsa-mir-29c', 'hsa-mir-330', 'hsa-mir-505', 'hsa-mir-204', 'hsa-mir-205', 'hsa-mir-96', 'hsa-mir-519d', 'hsa-mir-99a', 'hsa-mir-98', 'hsa-mir-16-2', 'hsa-mir-1180', 'hsa-mir-133a', 'hsa-mir-374c', 'hsa-mir-500a', 'ebv-miR-BART11', 'hsa-mir-552', 'hsa-mir-93', 'hsa-mir-377', 'hsa-let-7b', 'hsa-mir-200c', 'ebv-miR-BART3', 'hsa-mir-425', 'hsa-mir-4787', 'hsa-mir-1271', 'hsa-mir-653', 'hsa-mir-548y', 'hsa-mir-548z', 'hsa-mir-23b', 'hsa-mir-1260b', 'hsa-mir-4690', 'hsa-mir-298', 'hsa-mir-625', 'hsa-mir-10a', 'hsa-mir-1226', 'hsa-mir-624', 'hsa-mir-708', 'hsa-mir-23a', 'hsa-mir-191', 'hsa-mir-181d', 'hsa-mir-320c', 'hsa-mir-106a', 'hsa-mir-450b', 'hsa-mir-671', 'hsa-mir-26b', 'hsa-mir-1269a', 'hsa-mir-449a', 'hsa-mir-127', 'hsa-mir-1207', 'hsa-mir-423', 'hsa-mir-346', 'hsa-mir-17', 'hsa-mir-606', 'hsa-mir-137', 'hsa-mir-548c', 'hsa-mir-520a', 'hsa-mir-1', 'hsa-mir-3179', 'hsa-let-7a', 'hsa-mir-510', 'hsa-mir-1246', 'hsa-mir-19b-1', 'hsa-mir-203', 'hsa-mir-5100', 'hsa-mir-132', 'hsa-mir-362', 'hsa-mir-335', 'hsa-mir-877', 'kshv-miR-K12-1*', 'hsa-mir-1202', 'hsa-mir-651', 'hsa-mir-133b', 'kshv-miR-K12-12', 'hsa-mir-149', 'hsa-mir-522', 'hsa-mir-29a', 'hsa-let-7f', 'hsa-mir-548d', 'hsa-mir-514a', 'hsa-mir-125a', 'hsa-mir-193b', 'hsa-mir-1296', 'hsa-mir-7', 'hsa-mir-301a', 'hsa-mir-100', 'hsa-mir-1268a', 'hsa-mir-532', 'hsa-mir-654', 'hsa-mir-520b', 'hsa-mir-197', 'hsa-mir-1913', 'kshv-miR-K12-6', 'hsa-mir-20a', 'hsa-mir-374b', 'hsa-mir-185', 'hsa-mir-501', 'hsa-mir-28', 'ebv-miR-BART10', 'hsa-mir-22', 'hsa-mir-1179', 'hsa-mir-320a', 'hsa-mir-338', 'hsa-mir-222', 'hsa-mir-644a', 'hsa-mir-3174', 'hsa-mir-3176', 'hsa-mir-331', 'hsa-mir-26a', 'hsa-mir-21', 'hsa-mir-25', 'hsa-mir-218', 'kshv-miR-K12-11', 'hsa-mir-182', 'hsa-mir-609', 'hsa-mir-148b', 'hsa-mir-3200', 'hsa-mir-195', 'hsa-mir-571', 'hsa-mir-1287', 'hsa-mir-188', 'hsa-mir-299', 'hsa-mir-200a', 'ebv-miR-BART18', 'hsa-mir-1234', 'hsa-mir-18b', 'hsa-mir-374a', 'ebv-miR-BART22', 'hsa-mir-449b', 'hsa-mir-301b', 'hsa-mir-4254', 'hsa-mir-485', 'hsa-let-7f-2', 'hsa-mir-148a', 'hsa-mir-634', 'hsa-mir-660', 'hsa-mir-378a', 'hsa-mir-27b', 'hsa-mir-1283', 'hsa-mir-130a', 'hsa-mir-1249', 'hsa-mir-33a', 'hsa-mir-380', 'hsa-mir-548h', 'hsa-mir-181a', 'hsa-mir-18a', 'hsa-mir-1228', 'hsa-mir-454', 'ebv-miR-BART17', 'hsa-mir-32', 'hsa-mir-597', 'hsa-mir-582', 'hsa-mir-29b-2', 'hsa-mir-370', 'hsa-mir-181a-2', 'hsa-mir-548b', 'hsa-mir-520e', 'hsa-mir-141', 'hsa-mir-885', 'hsa-mir-452', 'kshv-miR-K12-7*', 'hsa-mir-371a', 'hsa-mir-483', 'hsa-mir-107', 'hsa-mir-542', 'hsa-mir-3679', 'hsa-mir-16-1', 'hsa-mir-760', 'hsa-mir-198', 'hsa-mir-659', 'ebv-miR-BART15', 'hsa-mir-935', 'hsa-mir-320b', 'hsa-mir-181b', 'hsa-mir-129-1', 'hsa-mir-548j', 'hsa-mir-432']\n"
     ]
    }
   ],
   "source": [
    "#mirna-pathways annotation\n",
    "# miRNA\n",
    "path = './KEGG_pathways/kegg_anano.txt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_mirna_dict = {}\n",
    "j=1\n",
    "for i in files:\n",
    "     keys = i.split(',')[0].split('|')[1]\n",
    "     values1 = i.split(',')[1:-1]\n",
    "     values2 =  i.split(',')[-1].replace('\\n','')\n",
    "     values1.append(values2)\n",
    "     values1 =list(set(values1)) \n",
    "     paways_mirna_dict[keys] = values1\n",
    "     if(j==1):\n",
    "          print(keys)\n",
    "          print(paways_mirna_dict[keys])\n",
    "          j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27293b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "#两个集合交集\n",
    "#同时与基因和miRNA相关的通路数量\n",
    "union_kegg = list(set(paways_genes_dict.keys()).intersection(set(paways_mirna_dict.keys())))\n",
    "print(len(union_kegg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cde7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#只保留同时与基因和miRNA相关的通路\n",
    "paways_genes_dicts ={}\n",
    "paways_mirna_dicts ={}\n",
    "\n",
    "for i in union_kegg:\n",
    "    paways_genes_dicts[i] = paways_genes_dict[i]\n",
    "    \n",
    "for i in union_kegg:\n",
    "    paways_mirna_dicts[i] = paways_mirna_dict[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fe7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genes_existed_pathway = []\n",
    "#交集通路中基因相关的基因\n",
    "mirna_existed_pathway = []\n",
    "#交集通路中miRNA相关的miRNA\n",
    "for index in paways_genes_dicts.keys():\n",
    "    genes_existed_pathway = genes_existed_pathway+ list(paways_genes_dicts[index])\n",
    "genes_existed_pathway = set(genes_existed_pathway)\n",
    "\n",
    "\n",
    "for index in paways_mirna_dicts.keys():\n",
    "    mirna_existed_pathway = mirna_existed_pathway+ list(paways_mirna_dicts[index])\n",
    "mirna_existed_pathway = set(mirna_existed_pathway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8c874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7768\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "print(len(genes_existed_pathway))\n",
    "print(len(mirna_existed_pathway))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3877bbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "miRNA_data = pd.read_csv(\"./AML_data/miRNA_data.csv\",index_col = 0)\n",
    "\n",
    "mRNA_data = pd.read_csv(\"./AML_data/mRNA_data.csv\",index_col = 0)\n",
    "\n",
    "example_case = pd.read_csv('./AML_data/response.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8a004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 2000)\n",
      "(221, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mRNA_data.shape)\n",
    "print(miRNA_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165fe600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRNA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1967e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5295fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_gene_miRNA = list(miRNA_data.columns)\n",
    "union_gene_mRNA = list(mRNA_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b65eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_union = list(paways_genes_dicts.keys())\n",
    "len(pathway_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0608bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = [union_gene_mRNA]\n",
    "#包含各类组学数据的特征名称列表，这里是union_gene_mRNA，表示mRNA组学的基因名称列表\n",
    "gene_pathway_bp_dfs = []\n",
    "#用于存储构建的关系矩阵的列表\n",
    "\n",
    "for i in range(len(mask_list)):\n",
    "    pathways_genes = np.zeros((len(pathway_union), len(mask_list[i]))) \n",
    "    #初始化的全零矩阵，行数为通路数量，列数为基因数量\n",
    "\n",
    "    for p  in pathway_union:\n",
    "        gs = paways_genes_dicts[p]\n",
    "        #paways_genes_dicts：字典数据结构，键是通路ID，值是属于该通路的基因列表\n",
    "        #gs：当前通路p对应的基因列表\n",
    "        g_inds = [mask_list[i].index(g) for g in gs if g in mask_list[i]]\n",
    "        #g_inds：当前通路p对应的基因在mask_list[i]中的索引列表\n",
    "        p_ind = pathway_union.index(p)\n",
    "        #p_ind：当前通路p在pathway_union中的索引\n",
    "        pathways_genes[p_ind, g_inds] = 1\n",
    "    gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=mask_list[i])\n",
    "    #最终构建的关系矩阵，行是通路，列是基因，值为1表示该基因属于该通路\n",
    "    \n",
    "    gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "#这里gene_pathway_bp_dfs是基因-通路矩阵,也就是先验知识\n",
    "    \n",
    "\n",
    "pathways_genes = np.zeros((len(pathway_union), len(union_gene_miRNA))) \n",
    "#初始化全零矩阵，行数为通路数量，列数为miRNA数量\n",
    "for p  in pathway_union:\n",
    "    gs = paways_mirna_dicts[p]\n",
    "    #paways_mirna_dicts：字典数据结构，键是通路ID，值是属于该通路的miRNA列表\n",
    "    #gs：当前通路p对应的miRNA列表\n",
    "    g_inds = [union_gene_miRNA.index(g) for g in gs if g in union_gene_miRNA]\n",
    "    #g_inds：当前通路p对应的miRNA在union_gene_miRNA中的索引列表\n",
    "    p_ind = pathway_union.index(p)\n",
    "    pathways_genes[p_ind, g_inds] = 1\n",
    "gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=union_gene_miRNA)\n",
    "#最终构建的关系矩阵，行是通路，列是miRNA，值为1表示该miRNA属于该通路\n",
    "gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a9d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gene_pathway_bp_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15821250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_pathway_bp_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125e5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import glorot_uniform, Initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D,Layer\n",
    "from tensorflow.keras import initializers,activations,regularizers\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    " \n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biological_module(Layer):\n",
    "    def __init__(self, units, mapp=None, nonzero_ind=None, kernel_initializer='glorot_uniform', W_regularizer=None,\n",
    "                 activation='tanh', use_bias=True,bias_initializer='zeros', bias_regularizer=None,\n",
    "                 bias_constraint=None,**kwargs):\n",
    "        \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.mapp = mapp\n",
    "        self.nonzero_ind = nonzero_ind\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activation_fn = activations.get(activation)\n",
    "        super(Biological_module, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        input_dim = input_shape[1]\n",
    "   \n",
    "\n",
    "        if not self.mapp is None:\n",
    "            self.mapp = self.mapp.astype(np.float32)\n",
    "\n",
    "   \n",
    "        if self.nonzero_ind is None:\n",
    "            nonzero_ind = np.array(np.nonzero(self.mapp)).T\n",
    "            self.nonzero_ind = nonzero_ind\n",
    "\n",
    "        self.kernel_shape = (input_dim, self.units)\n",
    "        \n",
    "\n",
    "        nonzero_count = self.nonzero_ind.shape[0]   \n",
    "\n",
    "\n",
    "        self.kernel_vector = self.add_weight(name='kernel_vector',\n",
    "                                             shape=(nonzero_count,),\n",
    "                                             initializer=self.kernel_initializer,\n",
    "                                             regularizer=self.kernel_regularizer,\n",
    "                                             trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer\n",
    "                                        )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        super(Biological_module, self).build(input_shape)  \n",
    "      \n",
    "\n",
    "    def call(self, inputs):# 前向传播\n",
    "        \n",
    "        ## 从非零索引重建关系矩阵\n",
    "        trans = tf.scatter_nd(tf.constant(self.nonzero_ind, tf.int32), self.kernel_vector,\n",
    "                           tf.constant(list(self.kernel_shape)))\n",
    "        ## 实现公式2中的矩阵乘法 X^z * M^z\n",
    "        output = K.dot(inputs, trans)\n",
    "        \n",
    "        ## 如果使用偏置，则添加偏置\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "            \n",
    "        ## 如果激活函数不为空，则应用激活函数\n",
    "        if self.activation_fn is not None:\n",
    "            output = self.activation_fn(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': self.activation,\n",
    "            'use_bias': self.use_bias,\n",
    "            'nonzero_ind': np.array(self.nonzero_ind),\n",
    "          \n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'W_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "\n",
    "        }\n",
    "        base_config = super(Biological_module, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "      \n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c438cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    " \n",
    "    def __init__(self, output_dim,  W_regularizer=None,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      trainable=True)\n",
    " \n",
    "        super(Self_Attention, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        #self.kernel是一个形状为(3, input_shape[1], output_dim)\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    " \n",
    "\n",
    "        print(\"K.permute_dimensions(WK.shape\",(K.permute_dimensions(WK,[1,0]).shape))\n",
    " \n",
    "        # 计算注意力分数\n",
    "        QK =  K.dot(K.permute_dimensions(WK,[1,0]),WQ)\n",
    "    \n",
    "        ## 缩放除以sqrt(d_k)，对应公式(3)中的根号d_k\n",
    "        QK = QK / (64**0.5)\n",
    " \n",
    "        QK = K.softmax(QK)\n",
    " \n",
    "        print(\"QK.shape\",QK.shape) ## 这里的QK就是公式3中的α_ij矩阵\n",
    " \n",
    "        # 利用注意力分数对值矩阵进行加权\n",
    "        V = K.dot(WV, QK)  # 将注意力分数应用到值矩阵上\n",
    "        \n",
    "        print(V.shape)\n",
    " \n",
    "        return V\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          \n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "\n",
    "        }\n",
    "        base_config = super(Self_Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    " \n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020e30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mRNA_data,miRNA_data):\n",
    "    \n",
    "\n",
    "    \n",
    "    S_inputs_mRNA = Input(shape=(mRNA_data.shape[1],), dtype='float32',name= 'mRNA_inputs')\n",
    "  \n",
    "    S_inputs_miRNA = Input(shape=(miRNA_data.shape[1],), dtype='float32',name= 'miRNA_inputs')\n",
    "    \n",
    "\n",
    "   \n",
    "    h0_mRNA = Biological_module(gene_pathway_bp_dfs[0].shape[0],mapp =gene_pathway_bp_dfs[0].values.T, name = 'h0_mRNA',W_regularizer=l2(0.001))(S_inputs_mRNA)\n",
    "    #W_regularizer=l2(0.001)就是正则化项\n",
    "    \n",
    "    h0_miRNA = Biological_module(gene_pathway_bp_dfs[1].shape[0],mapp =gene_pathway_bp_dfs[1].values.T, name = 'h0_miRNA',W_regularizer=l2(0.001))(S_inputs_miRNA)\n",
    "\n",
    "    atten1 = Self_Attention(64,W_regularizer=l2(0.003))(h0_mRNA)\n",
    "    atten2 = Self_Attention(64,W_regularizer=l2(0.003))(h0_miRNA)\n",
    "    \n",
    "    feature_tal = tf.keras.layers.concatenate([atten1,atten2])\n",
    "    #连接起来\n",
    "\n",
    "    #MLP\n",
    "    h4 = tf.keras.layers.Dense(32,activation='tanh')(feature_tal)\n",
    "    \n",
    "    h5 = tf.keras.layers.Dense(1,activation='sigmoid')(h4)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[S_inputs_mRNA,S_inputs_miRNA], outputs=h5)\n",
    "\n",
    "    model.summary()\n",
    "    # 使用Adam优化器，学习率0.0001，衰减0.0001\n",
    "    # 使用二元交叉熵损失函数，评估指标为准确率\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.0001,decay=0.0001) \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1044dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "   \n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def get_metrics(true_score,pre_score,pre_probe):\n",
    "    \n",
    "  \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_score, pre_probe, pos_label=1)\n",
    "   \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    aupr = average_precision_score(true_score, pre_probe)\n",
    "    \n",
    "    pre, rec, thresholds = precision_recall_curve(true_score, pre_probe)    \n",
    "    auprc  = metrics.auc(rec, pre)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(true_score,pre_score)\n",
    "    \n",
    "    f1 = metrics.f1_score(true_score, pre_score)\n",
    "    \n",
    "    precision = metrics.precision_score(true_score,pre_score)\n",
    "    \n",
    "    recall = metrics.recall_score(true_score,pre_score)\n",
    "    \n",
    "     \n",
    "    print( print(confusion_matrix(true_score,pre_score)))\n",
    "    return precision,accuracy,recall,f1,auc,aupr,auprc\n",
    "\n",
    "\n",
    "def evaluates(y_test, y_pred):\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    \n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)    \n",
    "    auprc  = metrics.auc(recall, precision)\n",
    "    \n",
    "    pp = [1 if index>=0.5  else 0 for index in  y_pred ]\n",
    "    \n",
    "    pre = metrics.precision_score(y_test,pp)\n",
    "    \n",
    "    f1 = metrics.f1_score(y_test,pp)\n",
    "    \n",
    "    rec = metrics.recall_score(y_test,pp)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test,pp)\n",
    "    \n",
    "    print(confusion_matrix(y_test,pp))\n",
    "    return pre,acc,rec,f1,auc,aupr,auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cad9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 120 101\n",
      "1.0940594059405941 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "y = example_case['response'].values\n",
    "n_samples =example_case['response'].values\n",
    "#221个样本 120个阳性样本 101个阴性样本\n",
    "print(len(n_samples),n_samples.sum(),(len(n_samples) -n_samples.sum()))\n",
    "#计算权重\n",
    "x_0 =  len(n_samples) / (2*  (len(n_samples) -n_samples.sum()))\n",
    "x_1 =  len(n_samples) / (2*  n_samples.sum())\n",
    "\n",
    "print(x_0,x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80621e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\10263\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention (Self_Attention (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_1 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           self__attention[0][0]            \n",
      "                                                                 self__attention_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           4128        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 176 samples\n",
      "Epoch 1/130\n",
      "176/176 [==============================] - 0s 848us/sample - loss: 0.9225 - acc: 0.5625\n",
      "Epoch 2/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.9183 - acc: 0.5568\n",
      "Epoch 3/130\n",
      "176/176 [==============================] - 0s 84us/sample - loss: 0.9140 - acc: 0.5739\n",
      "Epoch 4/130\n",
      "176/176 [==============================] - 0s 87us/sample - loss: 0.9098 - acc: 0.6023\n",
      "Epoch 5/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.9055 - acc: 0.6705\n",
      "Epoch 6/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.9014 - acc: 0.6761\n",
      "Epoch 7/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8972 - acc: 0.6989\n",
      "Epoch 8/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8930 - acc: 0.7102\n",
      "Epoch 9/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.8890 - acc: 0.7159\n",
      "Epoch 10/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.8849 - acc: 0.7216\n",
      "Epoch 11/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.8807 - acc: 0.7216\n",
      "Epoch 12/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.8766 - acc: 0.7045\n",
      "Epoch 13/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.8725 - acc: 0.6989\n",
      "Epoch 14/130\n",
      "176/176 [==============================] - 0s 73us/sample - loss: 0.8682 - acc: 0.7045\n",
      "Epoch 15/130\n",
      "176/176 [==============================] - 0s 65us/sample - loss: 0.8640 - acc: 0.7102\n",
      "Epoch 16/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8598 - acc: 0.7045\n",
      "Epoch 17/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.8553 - acc: 0.6932\n",
      "Epoch 18/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8507 - acc: 0.7045\n",
      "Epoch 19/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.8461 - acc: 0.7102\n",
      "Epoch 20/130\n",
      "176/176 [==============================] - 0s 73us/sample - loss: 0.8412 - acc: 0.7045\n",
      "Epoch 21/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.8361 - acc: 0.7102\n",
      "Epoch 22/130\n",
      "176/176 [==============================] - 0s 101us/sample - loss: 0.8309 - acc: 0.7102\n",
      "Epoch 23/130\n",
      "176/176 [==============================] - 0s 97us/sample - loss: 0.8252 - acc: 0.7102\n",
      "Epoch 24/130\n",
      "176/176 [==============================] - 0s 98us/sample - loss: 0.8192 - acc: 0.7102\n",
      "Epoch 25/130\n",
      "176/176 [==============================] - 0s 104us/sample - loss: 0.8128 - acc: 0.7045\n",
      "Epoch 26/130\n",
      "176/176 [==============================] - 0s 94us/sample - loss: 0.8061 - acc: 0.7102\n",
      "Epoch 27/130\n",
      "176/176 [==============================] - 0s 111us/sample - loss: 0.7985 - acc: 0.7273\n",
      "Epoch 28/130\n",
      "176/176 [==============================] - 0s 97us/sample - loss: 0.7908 - acc: 0.7159\n",
      "Epoch 29/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.7832 - acc: 0.7273\n",
      "Epoch 30/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7743 - acc: 0.7330\n",
      "Epoch 31/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7652 - acc: 0.7330\n",
      "Epoch 32/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7563 - acc: 0.7330\n",
      "Epoch 33/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.7475 - acc: 0.7386\n",
      "Epoch 34/130\n",
      "176/176 [==============================] - 0s 89us/sample - loss: 0.7373 - acc: 0.7386\n",
      "Epoch 35/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.7284 - acc: 0.7386\n",
      "Epoch 36/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.7182 - acc: 0.7386\n",
      "Epoch 37/130\n",
      "176/176 [==============================] - 0s 92us/sample - loss: 0.7087 - acc: 0.7443\n",
      "Epoch 38/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.7005 - acc: 0.7386\n",
      "Epoch 39/130\n",
      "176/176 [==============================] - 0s 110us/sample - loss: 0.6910 - acc: 0.7330\n",
      "Epoch 40/130\n",
      "176/176 [==============================] - 0s 117us/sample - loss: 0.6822 - acc: 0.7386\n",
      "Epoch 41/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.6739 - acc: 0.7386\n",
      "Epoch 42/130\n",
      "176/176 [==============================] - 0s 131us/sample - loss: 0.6660 - acc: 0.7386\n",
      "Epoch 43/130\n",
      "176/176 [==============================] - 0s 134us/sample - loss: 0.6595 - acc: 0.7330\n",
      "Epoch 44/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.6506 - acc: 0.7386\n",
      "Epoch 45/130\n",
      "176/176 [==============================] - 0s 153us/sample - loss: 0.6444 - acc: 0.7386\n",
      "Epoch 46/130\n",
      "176/176 [==============================] - 0s 148us/sample - loss: 0.6378 - acc: 0.7443\n",
      "Epoch 47/130\n",
      "176/176 [==============================] - 0s 128us/sample - loss: 0.6316 - acc: 0.7443\n",
      "Epoch 48/130\n",
      "176/176 [==============================] - 0s 142us/sample - loss: 0.6259 - acc: 0.7500\n",
      "Epoch 49/130\n",
      "176/176 [==============================] - 0s 140us/sample - loss: 0.6204 - acc: 0.7557\n",
      "Epoch 50/130\n",
      "176/176 [==============================] - 0s 137us/sample - loss: 0.6156 - acc: 0.7614\n",
      "Epoch 51/130\n",
      "176/176 [==============================] - 0s 148us/sample - loss: 0.6094 - acc: 0.7500\n",
      "Epoch 52/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.6045 - acc: 0.7500\n",
      "Epoch 53/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.5991 - acc: 0.7614\n",
      "Epoch 54/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.5948 - acc: 0.7557\n",
      "Epoch 55/130\n",
      "176/176 [==============================] - 0s 89us/sample - loss: 0.5902 - acc: 0.7614\n",
      "Epoch 56/130\n",
      "176/176 [==============================] - 0s 100us/sample - loss: 0.5852 - acc: 0.7614\n",
      "Epoch 57/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5806 - acc: 0.7614\n",
      "Epoch 58/130\n",
      "176/176 [==============================] - 0s 122us/sample - loss: 0.5761 - acc: 0.7614\n",
      "Epoch 59/130\n",
      "176/176 [==============================] - 0s 112us/sample - loss: 0.5721 - acc: 0.7670\n",
      "Epoch 60/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.5675 - acc: 0.7670\n",
      "Epoch 61/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.5628 - acc: 0.7670\n",
      "Epoch 62/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.5587 - acc: 0.7727\n",
      "Epoch 63/130\n",
      "176/176 [==============================] - 0s 88us/sample - loss: 0.5545 - acc: 0.7727\n",
      "Epoch 64/130\n",
      "176/176 [==============================] - 0s 90us/sample - loss: 0.5503 - acc: 0.7784\n",
      "Epoch 65/130\n",
      "176/176 [==============================] - 0s 99us/sample - loss: 0.5470 - acc: 0.7727\n",
      "Epoch 66/130\n",
      "176/176 [==============================] - 0s 139us/sample - loss: 0.5416 - acc: 0.7784\n",
      "Epoch 67/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.5377 - acc: 0.7784\n",
      "Epoch 68/130\n",
      "176/176 [==============================] - 0s 136us/sample - loss: 0.5334 - acc: 0.7727\n",
      "Epoch 69/130\n",
      "176/176 [==============================] - 0s 99us/sample - loss: 0.5297 - acc: 0.7727\n",
      "Epoch 70/130\n",
      "176/176 [==============================] - 0s 117us/sample - loss: 0.5266 - acc: 0.7727\n",
      "Epoch 71/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.5216 - acc: 0.7784\n",
      "Epoch 72/130\n",
      "176/176 [==============================] - 0s 131us/sample - loss: 0.5174 - acc: 0.7784\n",
      "Epoch 73/130\n",
      "176/176 [==============================] - 0s 148us/sample - loss: 0.5137 - acc: 0.7841\n",
      "Epoch 74/130\n",
      "176/176 [==============================] - 0s 139us/sample - loss: 0.5099 - acc: 0.8011\n",
      "Epoch 75/130\n",
      "176/176 [==============================] - 0s 124us/sample - loss: 0.5063 - acc: 0.7955\n",
      "Epoch 76/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.5020 - acc: 0.7955\n",
      "Epoch 77/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.4991 - acc: 0.8011\n",
      "Epoch 78/130\n",
      "176/176 [==============================] - 0s 230us/sample - loss: 0.4944 - acc: 0.8011\n",
      "Epoch 79/130\n",
      "176/176 [==============================] - 0s 131us/sample - loss: 0.4912 - acc: 0.8068\n",
      "Epoch 80/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4878 - acc: 0.8011\n",
      "Epoch 81/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.4842 - acc: 0.8068\n",
      "Epoch 82/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4802 - acc: 0.8068\n",
      "Epoch 83/130\n",
      "176/176 [==============================] - 0s 92us/sample - loss: 0.4768 - acc: 0.8068\n",
      "Epoch 84/130\n",
      "176/176 [==============================] - 0s 96us/sample - loss: 0.4732 - acc: 0.8068\n",
      "Epoch 85/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.4697 - acc: 0.8125\n",
      "Epoch 86/130\n",
      "176/176 [==============================] - 0s 94us/sample - loss: 0.4662 - acc: 0.8125\n",
      "Epoch 87/130\n",
      "176/176 [==============================] - 0s 137us/sample - loss: 0.4626 - acc: 0.8125\n",
      "Epoch 88/130\n",
      "176/176 [==============================] - 0s 151us/sample - loss: 0.4620 - acc: 0.8068\n",
      "Epoch 89/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.4565 - acc: 0.8125\n",
      "Epoch 90/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.4529 - acc: 0.8239\n",
      "Epoch 91/130\n",
      "176/176 [==============================] - 0s 147us/sample - loss: 0.4500 - acc: 0.8239\n",
      "Epoch 92/130\n",
      "176/176 [==============================] - 0s 125us/sample - loss: 0.4460 - acc: 0.8239\n",
      "Epoch 93/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.4435 - acc: 0.8295\n",
      "Epoch 94/130\n",
      "176/176 [==============================] - 0s 117us/sample - loss: 0.4401 - acc: 0.8295\n",
      "Epoch 95/130\n",
      "176/176 [==============================] - 0s 136us/sample - loss: 0.4366 - acc: 0.8239\n",
      "Epoch 96/130\n",
      "176/176 [==============================] - 0s 131us/sample - loss: 0.4341 - acc: 0.8239\n",
      "Epoch 97/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.4305 - acc: 0.8239\n",
      "Epoch 98/130\n",
      "176/176 [==============================] - 0s 130us/sample - loss: 0.4275 - acc: 0.8239\n",
      "Epoch 99/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.4242 - acc: 0.8295\n",
      "Epoch 100/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.4211 - acc: 0.8295\n",
      "Epoch 101/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.4182 - acc: 0.8295\n",
      "Epoch 102/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.4148 - acc: 0.8295\n",
      "Epoch 103/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.4118 - acc: 0.8295\n",
      "Epoch 104/130\n",
      "176/176 [==============================] - 0s 108us/sample - loss: 0.4086 - acc: 0.8295\n",
      "Epoch 105/130\n",
      "176/176 [==============================] - 0s 122us/sample - loss: 0.4063 - acc: 0.8295\n",
      "Epoch 106/130\n",
      "176/176 [==============================] - 0s 131us/sample - loss: 0.4033 - acc: 0.8295\n",
      "Epoch 107/130\n",
      "176/176 [==============================] - 0s 99us/sample - loss: 0.3997 - acc: 0.8352\n",
      "Epoch 108/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.3973 - acc: 0.8409\n",
      "Epoch 109/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.3943 - acc: 0.8409\n",
      "Epoch 110/130\n",
      "176/176 [==============================] - 0s 94us/sample - loss: 0.3913 - acc: 0.8352\n",
      "Epoch 111/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.3879 - acc: 0.8466\n",
      "Epoch 112/130\n",
      "176/176 [==============================] - 0s 97us/sample - loss: 0.3858 - acc: 0.8466\n",
      "Epoch 113/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.3820 - acc: 0.8580\n",
      "Epoch 114/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.3799 - acc: 0.8580\n",
      "Epoch 115/130\n",
      "176/176 [==============================] - 0s 103us/sample - loss: 0.3774 - acc: 0.8523\n",
      "Epoch 116/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.3739 - acc: 0.8693\n",
      "Epoch 117/130\n",
      "176/176 [==============================] - 0s 125us/sample - loss: 0.3712 - acc: 0.8750\n",
      "Epoch 118/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.3681 - acc: 0.8750\n",
      "Epoch 119/130\n",
      "176/176 [==============================] - 0s 109us/sample - loss: 0.3662 - acc: 0.8750\n",
      "Epoch 120/130\n",
      "176/176 [==============================] - 0s 103us/sample - loss: 0.3625 - acc: 0.8750\n",
      "Epoch 121/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.3595 - acc: 0.8807\n",
      "Epoch 122/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.3569 - acc: 0.8807\n",
      "Epoch 123/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.3540 - acc: 0.8807\n",
      "Epoch 124/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.3522 - acc: 0.8807\n",
      "Epoch 125/130\n",
      "176/176 [==============================] - 0s 103us/sample - loss: 0.3488 - acc: 0.8807\n",
      "Epoch 126/130\n",
      "176/176 [==============================] - 0s 105us/sample - loss: 0.3466 - acc: 0.8807\n",
      "Epoch 127/130\n",
      "176/176 [==============================] - 0s 111us/sample - loss: 0.3434 - acc: 0.8864\n",
      "Epoch 128/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.3405 - acc: 0.8807\n",
      "Epoch 129/130\n",
      "176/176 [==============================] - 0s 119us/sample - loss: 0.3382 - acc: 0.8864\n",
      "Epoch 130/130\n",
      "176/176 [==============================] - 0s 122us/sample - loss: 0.3351 - acc: 0.8920\n",
      "[[16  5]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8, 0.8, 0.8333333333333334, 0.816326530612245, 0.8511904761904762, 0.8946889739682342, 0.8926091390735972)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_2 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_3 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           self__attention_2[0][0]          \n",
      "                                                                 self__attention_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 545us/sample - loss: 0.9227 - acc: 0.5763\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.9185 - acc: 0.5480\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.9144 - acc: 0.5819\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.9102 - acc: 0.5989\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.9062 - acc: 0.6158\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.9020 - acc: 0.6441\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8981 - acc: 0.6610\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8941 - acc: 0.6441\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.8901 - acc: 0.6610\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8861 - acc: 0.6667\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8822 - acc: 0.6554\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8783 - acc: 0.6497\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.8714 - acc: 0.640 - 0s 102us/sample - loss: 0.8742 - acc: 0.6723\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 101us/sample - loss: 0.8703 - acc: 0.6836\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.8662 - acc: 0.7006\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.8621 - acc: 0.7006\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 141us/sample - loss: 0.8579 - acc: 0.7062\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.8536 - acc: 0.7175\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 176us/sample - loss: 0.8492 - acc: 0.7232\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 144us/sample - loss: 0.8448 - acc: 0.7175\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8401 - acc: 0.7175\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8354 - acc: 0.7006\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.8303 - acc: 0.7006\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.8251 - acc: 0.7006\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8196 - acc: 0.7119\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8138 - acc: 0.7119\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8074 - acc: 0.7062\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.8012 - acc: 0.7119\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.7943 - acc: 0.7288\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 134us/sample - loss: 0.7870 - acc: 0.7401\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.7794 - acc: 0.7458\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.7711 - acc: 0.7514\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.7629 - acc: 0.7514\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 152us/sample - loss: 0.7539 - acc: 0.7514\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 161us/sample - loss: 0.7453 - acc: 0.7627\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 147us/sample - loss: 0.7358 - acc: 0.7627\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 163us/sample - loss: 0.7273 - acc: 0.7627\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 179us/sample - loss: 0.7173 - acc: 0.7627\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 187us/sample - loss: 0.7078 - acc: 0.7684\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 198us/sample - loss: 0.6982 - acc: 0.7684\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 167us/sample - loss: 0.6889 - acc: 0.7684\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.6801 - acc: 0.7684\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.6710 - acc: 0.7684\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 148us/sample - loss: 0.6622 - acc: 0.7684\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 144us/sample - loss: 0.6540 - acc: 0.7740\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.6454 - acc: 0.7684\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6381 - acc: 0.7740\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.6310 - acc: 0.7797\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 125us/sample - loss: 0.6236 - acc: 0.7797\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 153us/sample - loss: 0.6163 - acc: 0.7853\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 149us/sample - loss: 0.6106 - acc: 0.7797\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.6039 - acc: 0.7797\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5977 - acc: 0.7853\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5917 - acc: 0.7853\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5875 - acc: 0.7853\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5805 - acc: 0.7853\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5746 - acc: 0.7853\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5695 - acc: 0.7966\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5654 - acc: 0.7966\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.5597 - acc: 0.8023\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.5544 - acc: 0.8079\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5487 - acc: 0.8023\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.5451 - acc: 0.7966\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 121us/sample - loss: 0.5409 - acc: 0.8023\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.5348 - acc: 0.8079\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5295 - acc: 0.8079\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.5252 - acc: 0.8136\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.5204 - acc: 0.8136\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.5161 - acc: 0.8192\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5117 - acc: 0.8192\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 117us/sample - loss: 0.5078 - acc: 0.8136\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.5028 - acc: 0.8192\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.4988 - acc: 0.8192\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 129us/sample - loss: 0.4944 - acc: 0.8249\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4900 - acc: 0.8305\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4863 - acc: 0.8362\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.4835 - acc: 0.8362\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.4782 - acc: 0.8305\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4756 - acc: 0.8362\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.4708 - acc: 0.8305\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 155us/sample - loss: 0.4670 - acc: 0.8362\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.4636 - acc: 0.8418\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4596 - acc: 0.8418\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.4565 - acc: 0.8418\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4534 - acc: 0.8418\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.4499 - acc: 0.8475\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4478 - acc: 0.8531\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4425 - acc: 0.8531\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 147us/sample - loss: 0.4389 - acc: 0.8531\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4369 - acc: 0.8531\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 146us/sample - loss: 0.4328 - acc: 0.8588\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4293 - acc: 0.8588\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4266 - acc: 0.8588\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4241 - acc: 0.8588\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 106us/sample - loss: 0.4209 - acc: 0.8588\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.4172 - acc: 0.8588\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4140 - acc: 0.8588\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4112 - acc: 0.8644\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.4081 - acc: 0.8644\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4052 - acc: 0.8644\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 147us/sample - loss: 0.4030 - acc: 0.8588\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.3999 - acc: 0.8588\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.3968 - acc: 0.8644\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 118us/sample - loss: 0.3938 - acc: 0.8701\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3913 - acc: 0.8701\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3889 - acc: 0.8757\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.3854 - acc: 0.8814\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.3827 - acc: 0.8870\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3807 - acc: 0.8870\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3775 - acc: 0.8983\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3748 - acc: 0.8983\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.3724 - acc: 0.8870\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3694 - acc: 0.8983\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3675 - acc: 0.8927\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3641 - acc: 0.9040\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3633 - acc: 0.9040\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3599 - acc: 0.8983\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.3565 - acc: 0.9040\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3548 - acc: 0.9040\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3518 - acc: 0.9040\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3487 - acc: 0.9040\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3471 - acc: 0.8927\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3450 - acc: 0.8927\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3422 - acc: 0.8927\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.3397 - acc: 0.9040\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3373 - acc: 0.9040\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.3346 - acc: 0.9040\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3323 - acc: 0.9096\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3314 - acc: 0.8983\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 125us/sample - loss: 0.3276 - acc: 0.8927\n",
      "[[13  7]\n",
      " [ 7 17]]\n",
      "None\n",
      "(0.7083333333333334, 0.6818181818181818, 0.7083333333333334, 0.7083333333333334, 0.7791666666666667, 0.8103199497280618, 0.8045420329543116)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_4 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_5 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           self__attention_4[0][0]          \n",
      "                                                                 self__attention_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           4128        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            33          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 925us/sample - loss: 0.9232 - acc: 0.4576\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.9189 - acc: 0.4576\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 302us/sample - loss: 0.9147 - acc: 0.4633\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9104 - acc: 0.4972\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.9062 - acc: 0.5763\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9020 - acc: 0.6384\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8978 - acc: 0.6384\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8936 - acc: 0.6610\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 138us/sample - loss: 0.8895 - acc: 0.7175\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8852 - acc: 0.7288\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8811 - acc: 0.7288\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.8768 - acc: 0.7288\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.8726 - acc: 0.7288\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.8683 - acc: 0.7288\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.8637 - acc: 0.7232\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 151us/sample - loss: 0.8592 - acc: 0.7345\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 149us/sample - loss: 0.8546 - acc: 0.7401\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 169us/sample - loss: 0.8497 - acc: 0.7401\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.8445 - acc: 0.7288\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 150us/sample - loss: 0.8392 - acc: 0.7288\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 166us/sample - loss: 0.8336 - acc: 0.7232\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 170us/sample - loss: 0.8277 - acc: 0.7232\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.8216 - acc: 0.7175\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.8148 - acc: 0.7232\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.8079 - acc: 0.7288\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.8007 - acc: 0.7288\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.7935 - acc: 0.7232\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 109us/sample - loss: 0.7849 - acc: 0.7232\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 198us/sample - loss: 0.7769 - acc: 0.7401\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 192us/sample - loss: 0.7685 - acc: 0.7401\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.7594 - acc: 0.7458\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.7515 - acc: 0.7345\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.7419 - acc: 0.7401\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.7332 - acc: 0.7232\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 123us/sample - loss: 0.7247 - acc: 0.7232\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.7159 - acc: 0.7232\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.7083 - acc: 0.7232\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 101us/sample - loss: 0.6998 - acc: 0.7345\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.6922 - acc: 0.7458\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.6857 - acc: 0.7458\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6780 - acc: 0.7458\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6710 - acc: 0.7458\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.6643 - acc: 0.7401\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.6592 - acc: 0.7458\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6525 - acc: 0.7345\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.6468 - acc: 0.7345\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6411 - acc: 0.7401\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6361 - acc: 0.7458\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 166us/sample - loss: 0.6309 - acc: 0.7458\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6257 - acc: 0.7458\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.6207 - acc: 0.7514\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.6162 - acc: 0.7514\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.6111 - acc: 0.7514\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.6068 - acc: 0.7627\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6023 - acc: 0.7627\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5977 - acc: 0.7571\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.5929 - acc: 0.7684\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5886 - acc: 0.7684\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5852 - acc: 0.7684\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5811 - acc: 0.7684\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5760 - acc: 0.7740\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5715 - acc: 0.7740\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.5691 - acc: 0.7740\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.5634 - acc: 0.7797\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.5593 - acc: 0.7797\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5557 - acc: 0.7853\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 144us/sample - loss: 0.5516 - acc: 0.7853\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 170us/sample - loss: 0.5476 - acc: 0.7853\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.5437 - acc: 0.7853\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5397 - acc: 0.7910\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.5359 - acc: 0.7740\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5323 - acc: 0.7740\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5279 - acc: 0.7797\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5238 - acc: 0.7910\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5205 - acc: 0.8023\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.5175 - acc: 0.7910\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 133us/sample - loss: 0.5131 - acc: 0.7910\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5096 - acc: 0.7910\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5055 - acc: 0.7966\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 147us/sample - loss: 0.5015 - acc: 0.8023\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 134us/sample - loss: 0.4978 - acc: 0.8079\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 192us/sample - loss: 0.4945 - acc: 0.8079\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4911 - acc: 0.8136\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.4876 - acc: 0.8136\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.4833 - acc: 0.8192\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4804 - acc: 0.8192\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.4766 - acc: 0.8192\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4737 - acc: 0.8192\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4693 - acc: 0.8305\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4676 - acc: 0.8305\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4627 - acc: 0.8305\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 189us/sample - loss: 0.4599 - acc: 0.8305\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 122us/sample - loss: 0.4568 - acc: 0.8305\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4532 - acc: 0.8305\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4495 - acc: 0.8305\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4464 - acc: 0.8362\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4429 - acc: 0.8362\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4413 - acc: 0.8475\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.4371 - acc: 0.8475\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4339 - acc: 0.8362\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4304 - acc: 0.8362\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4274 - acc: 0.8362\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4256 - acc: 0.8475\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4211 - acc: 0.8531\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4182 - acc: 0.8531\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.4150 - acc: 0.8588\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4120 - acc: 0.8644\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4090 - acc: 0.8644\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 161us/sample - loss: 0.4058 - acc: 0.8644\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4039 - acc: 0.8588\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4012 - acc: 0.8644\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3969 - acc: 0.8701\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 150us/sample - loss: 0.3947 - acc: 0.8701\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3916 - acc: 0.8701\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 156us/sample - loss: 0.3880 - acc: 0.8701\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 153us/sample - loss: 0.3852 - acc: 0.8701\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.3827 - acc: 0.8757\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.3797 - acc: 0.8757\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3764 - acc: 0.8757\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.3742 - acc: 0.8757\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3711 - acc: 0.8814\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.3690 - acc: 0.8757\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3654 - acc: 0.8870\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3626 - acc: 0.8870\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3601 - acc: 0.8870\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.3577 - acc: 0.8870\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.3542 - acc: 0.8870\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3516 - acc: 0.8870\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3486 - acc: 0.8870\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.3458 - acc: 0.8870\n",
      "[[16  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8181818181818182, 0.8333333333333334, 0.8333333333333334, 0.9041666666666667, 0.9182234492194824, 0.916054660344843)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_6 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_7 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           self__attention_6[0][0]          \n",
      "                                                                 self__attention_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           4128        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 707us/sample - loss: 0.9220 - acc: 0.4859\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.9178 - acc: 0.5819\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.9137 - acc: 0.6384\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.9096 - acc: 0.6610\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9055 - acc: 0.7006\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.9014 - acc: 0.6610\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.8974 - acc: 0.6554\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.8934 - acc: 0.6723\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.8895 - acc: 0.6667\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8855 - acc: 0.6554\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8815 - acc: 0.6610\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.8776 - acc: 0.6723\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.8736 - acc: 0.6836\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 118us/sample - loss: 0.8698 - acc: 0.6780\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8658 - acc: 0.6949\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.8618 - acc: 0.6893\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.8578 - acc: 0.6836\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8538 - acc: 0.6780\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8496 - acc: 0.6780\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8454 - acc: 0.6893\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8409 - acc: 0.6893\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.8366 - acc: 0.6949\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 143us/sample - loss: 0.8318 - acc: 0.7006\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 180us/sample - loss: 0.8272 - acc: 0.7119\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.8222 - acc: 0.7232\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.8166 - acc: 0.7175\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8113 - acc: 0.7288\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.8060 - acc: 0.7232\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.7999 - acc: 0.7345\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.7938 - acc: 0.7345\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 141us/sample - loss: 0.7872 - acc: 0.7345\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.7806 - acc: 0.7345\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.7735 - acc: 0.7288\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7660 - acc: 0.7401\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.7582 - acc: 0.7345\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 200us/sample - loss: 0.7502 - acc: 0.7401\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.7426 - acc: 0.7345\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.7339 - acc: 0.7345\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7268 - acc: 0.7288\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.7185 - acc: 0.7401\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.7106 - acc: 0.7345\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7028 - acc: 0.7401\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.6951 - acc: 0.7401\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6873 - acc: 0.7401\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 109us/sample - loss: 0.6808 - acc: 0.7401\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 118us/sample - loss: 0.6736 - acc: 0.7401\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 100us/sample - loss: 0.6669 - acc: 0.7401\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.6602 - acc: 0.7401\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6535 - acc: 0.7401\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6478 - acc: 0.7458\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.6420 - acc: 0.7458\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 121us/sample - loss: 0.6362 - acc: 0.7458\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 128us/sample - loss: 0.6303 - acc: 0.7401\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6244 - acc: 0.7514\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.6191 - acc: 0.7627\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.6140 - acc: 0.7627\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.6089 - acc: 0.7627\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.6040 - acc: 0.7627\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.5979 - acc: 0.7627\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5931 - acc: 0.7627\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.5880 - acc: 0.7684\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.5827 - acc: 0.7684\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 167us/sample - loss: 0.5782 - acc: 0.7740\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 108us/sample - loss: 0.5731 - acc: 0.7684\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 126us/sample - loss: 0.5683 - acc: 0.7684\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.5635 - acc: 0.7797\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5584 - acc: 0.7740\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5539 - acc: 0.7740\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5493 - acc: 0.7740\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.5440 - acc: 0.7853\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5397 - acc: 0.8023\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5370 - acc: 0.8023\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5316 - acc: 0.8023\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5263 - acc: 0.7910\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5222 - acc: 0.7966\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5188 - acc: 0.8023\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5130 - acc: 0.8023\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5104 - acc: 0.8136\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5047 - acc: 0.8136\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.5008 - acc: 0.8192\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4970 - acc: 0.8192\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 118us/sample - loss: 0.4932 - acc: 0.8192\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.4910 - acc: 0.8249\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 139us/sample - loss: 0.4850 - acc: 0.8249\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.4816 - acc: 0.8192\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4788 - acc: 0.8305\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 104us/sample - loss: 0.4746 - acc: 0.8305\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4711 - acc: 0.8305\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.4676 - acc: 0.8418\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.4636 - acc: 0.8418\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4601 - acc: 0.8418\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.4578 - acc: 0.8362\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4530 - acc: 0.8418\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.4507 - acc: 0.8305\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.4496 - acc: 0.8362\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4436 - acc: 0.8531\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 211us/sample - loss: 0.4403 - acc: 0.8588\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.4395 - acc: 0.8531\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4338 - acc: 0.8588\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 117us/sample - loss: 0.4307 - acc: 0.8588\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.4278 - acc: 0.8701\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.4251 - acc: 0.8757\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 151us/sample - loss: 0.4216 - acc: 0.8757\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 180us/sample - loss: 0.4190 - acc: 0.8814\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 145us/sample - loss: 0.4166 - acc: 0.8644\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.4131 - acc: 0.8701\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4097 - acc: 0.8870\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4085 - acc: 0.8870\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4052 - acc: 0.8870\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.4009 - acc: 0.8870\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.3982 - acc: 0.8870\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.3952 - acc: 0.8870\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3925 - acc: 0.8870\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3896 - acc: 0.8814\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.3871 - acc: 0.8870\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3845 - acc: 0.8870\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.3816 - acc: 0.8870\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3786 - acc: 0.8870\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.3765 - acc: 0.8870\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3739 - acc: 0.8870\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3718 - acc: 0.8870\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 153us/sample - loss: 0.3682 - acc: 0.8870\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.3657 - acc: 0.8870\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 133us/sample - loss: 0.3624 - acc: 0.8927\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 128us/sample - loss: 0.3598 - acc: 0.8927\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.3573 - acc: 0.8927\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.3550 - acc: 0.8927\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 109us/sample - loss: 0.3521 - acc: 0.8927\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3508 - acc: 0.8927\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.3471 - acc: 0.8870\n",
      "[[16  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8181818181818182, 0.8333333333333334, 0.8333333333333334, 0.9125, 0.9395653324025535, 0.9384500461249734)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_8 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_9 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           self__attention_8[0][0]          \n",
      "                                                                 self__attention_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           4128        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            33          dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 1ms/sample - loss: 0.9224 - acc: 0.4633\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.9181 - acc: 0.4746\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.9138 - acc: 0.5876\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.9095 - acc: 0.6328\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.9052 - acc: 0.6893\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.9010 - acc: 0.7006\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 152us/sample - loss: 0.8968 - acc: 0.7119\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.8926 - acc: 0.7062\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.8884 - acc: 0.7288\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.8842 - acc: 0.7006\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.8800 - acc: 0.7175\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.8758 - acc: 0.7119\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8715 - acc: 0.7006\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.8671 - acc: 0.6949\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8628 - acc: 0.7006\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8581 - acc: 0.6949\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.8536 - acc: 0.6949\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8490 - acc: 0.7119\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8438 - acc: 0.7062\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8388 - acc: 0.7119\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.8332 - acc: 0.7119\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.8276 - acc: 0.7232\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.8216 - acc: 0.7232\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.8152 - acc: 0.7288\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8086 - acc: 0.7345\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.8012 - acc: 0.7288\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.7940 - acc: 0.7288\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.7860 - acc: 0.7345\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.7775 - acc: 0.7345\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 153us/sample - loss: 0.7692 - acc: 0.7288\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.7602 - acc: 0.7232\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 141us/sample - loss: 0.7507 - acc: 0.7345\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 103us/sample - loss: 0.7416 - acc: 0.7288\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 201us/sample - loss: 0.7322 - acc: 0.7288\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.7234 - acc: 0.7288\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7140 - acc: 0.7288\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7053 - acc: 0.7288\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6969 - acc: 0.7288\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.6887 - acc: 0.7288\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.6805 - acc: 0.7345\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6729 - acc: 0.7458\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.6664 - acc: 0.7401\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 126us/sample - loss: 0.6587 - acc: 0.7458\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.6522 - acc: 0.7458\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.6461 - acc: 0.7514\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.6406 - acc: 0.7514\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 139us/sample - loss: 0.6343 - acc: 0.7514\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.6283 - acc: 0.7514\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.6225 - acc: 0.7514\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.6173 - acc: 0.7514\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 167us/sample - loss: 0.6132 - acc: 0.7627\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.6070 - acc: 0.7684\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.6020 - acc: 0.7627\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 132us/sample - loss: 0.5974 - acc: 0.7627\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5926 - acc: 0.7684\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 144us/sample - loss: 0.5884 - acc: 0.7684\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5834 - acc: 0.7740\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 147us/sample - loss: 0.5787 - acc: 0.7910\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 201us/sample - loss: 0.5737 - acc: 0.7910\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 147us/sample - loss: 0.5696 - acc: 0.7853\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5649 - acc: 0.7797\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.5613 - acc: 0.7910\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 122us/sample - loss: 0.5569 - acc: 0.7910\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 122us/sample - loss: 0.5531 - acc: 0.7910\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 170us/sample - loss: 0.5470 - acc: 0.7910\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 164us/sample - loss: 0.5429 - acc: 0.7910\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.5390 - acc: 0.8079\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5353 - acc: 0.8136\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5303 - acc: 0.8079\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5268 - acc: 0.8079\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.5221 - acc: 0.8192\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.5196 - acc: 0.8136\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5145 - acc: 0.8192\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.5105 - acc: 0.8079\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.5063 - acc: 0.8079\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5022 - acc: 0.8079\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4989 - acc: 0.8079\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4947 - acc: 0.8249\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4908 - acc: 0.8192\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4872 - acc: 0.8305\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 133us/sample - loss: 0.4837 - acc: 0.8305\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4796 - acc: 0.8249\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.4758 - acc: 0.8305\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4722 - acc: 0.8362\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 100us/sample - loss: 0.4686 - acc: 0.8418\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4658 - acc: 0.8418\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 165us/sample - loss: 0.4612 - acc: 0.8362\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4575 - acc: 0.8362\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4546 - acc: 0.8362\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.4512 - acc: 0.8362\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4474 - acc: 0.8362\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4453 - acc: 0.8418\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 146us/sample - loss: 0.4407 - acc: 0.8475\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 139us/sample - loss: 0.4370 - acc: 0.8588\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4338 - acc: 0.8588\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4309 - acc: 0.8531\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4271 - acc: 0.8588\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4236 - acc: 0.8588\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4201 - acc: 0.8588\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4172 - acc: 0.8531\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.4143 - acc: 0.8531\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 141us/sample - loss: 0.4106 - acc: 0.8531\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 158us/sample - loss: 0.4074 - acc: 0.8644\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.4040 - acc: 0.8701\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4011 - acc: 0.8701\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3977 - acc: 0.8701\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3945 - acc: 0.8701\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.3915 - acc: 0.8814\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3883 - acc: 0.8870\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3850 - acc: 0.8870\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3826 - acc: 0.8870\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3793 - acc: 0.8870\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3785 - acc: 0.8870\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 101us/sample - loss: 0.3735 - acc: 0.8870\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3695 - acc: 0.8927\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.3685 - acc: 0.8927\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 123us/sample - loss: 0.3654 - acc: 0.8983\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3608 - acc: 0.8927\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 133us/sample - loss: 0.3578 - acc: 0.8983\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3553 - acc: 0.9040\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3526 - acc: 0.9040\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3509 - acc: 0.9040\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3462 - acc: 0.9096\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3439 - acc: 0.9096\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 108us/sample - loss: 0.3403 - acc: 0.9096\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3377 - acc: 0.9096\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3353 - acc: 0.9096\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3321 - acc: 0.9096\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.3297 - acc: 0.9153\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.3268 - acc: 0.9209\n",
      "[[14  6]\n",
      " [ 5 19]]\n",
      "None\n",
      "(0.76, 0.75, 0.7916666666666666, 0.7755102040816326, 0.8166666666666668, 0.863693593816976, 0.8606746290984646)\n",
      "Cross validated results :  ACC = 0.7736363636363637, REC = 0.8, F1 = 0.7933673469387756, AUC = 0.8527380952380952, AUPR =0.8852982598270616\n"
     ]
    }
   ],
   "source": [
    "#Five-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1030) \n",
    "\n",
    "kfscore = []\n",
    "p = 0\n",
    "for train_index, test_index in skf.split(mRNA_data.values,y):\n",
    "\n",
    "\n",
    "    mRNA_train_x = mRNA_data.values[train_index]\n",
    "    mRNA_test_x  = mRNA_data.values[test_index]\n",
    "\n",
    "    miRNA_train_x = miRNA_data.values[train_index]\n",
    "    miRNA_test_x  = miRNA_data.values[test_index]\n",
    "\n",
    "    train_y  = y[train_index]\n",
    "    test_y   = y[test_index]\n",
    "\n",
    "    model = create_model(mRNA_data,miRNA_data)\n",
    "    model.fit( { \"mRNA_inputs\": mRNA_train_x, 'miRNA_inputs':miRNA_train_x},train_y,\n",
    "                 epochs=130,batch_size = 64,class_weight = {0:x_0,1:x_1})  \n",
    "\n",
    "    y_pred = model.predict({\"mRNA_inputs\": mRNA_test_x,'miRNA_inputs':miRNA_test_x})\n",
    "\n",
    "    y_score = [1 if index>=0.5  else 0 for index in  y_pred]\n",
    "\n",
    "    evaluate_epoch = get_metrics(test_y,y_score,y_pred)\n",
    "    print(evaluate_epoch)\n",
    "\n",
    "    kfscore.append(evaluate_epoch)\n",
    "    \n",
    "results = list(np.array(kfscore).sum(axis= 0)/5.0)\n",
    "print('Cross validated results :  ACC = {}, REC = {}, F1 = {}, AUC = {}, AUPR ={}'.format(results[1],results[2],results[3],results[4],results[5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepkngg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
