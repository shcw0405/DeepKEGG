{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094868a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ed8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes-pathways annotation\n",
    "\n",
    "path = './KEGG_pathways/20230205_kegg_hsa.gmt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_genes_dict = {}\n",
    "for i in files: \n",
    "    paways_genes_dict[i.split('\\t')[0].split('_')[0]] = i.replace('\\n','').split('\\t')[2:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-pathways annotation\n",
    "path = './KEGG_pathways/kegg_anano.txt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_mirna_dict = {}\n",
    "for i in files:\n",
    "     keys = i.split(',')[0].split('|')[1]\n",
    "     values1 = i.split(',')[1:-1]\n",
    "     values2 =  i.split(',')[-1].replace('\\n','')\n",
    "     values1.append(values2)\n",
    "     values1 =list(set(values1)) \n",
    "     paways_mirna_dict[keys] = values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27293b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_kegg = list(set(paways_genes_dict.keys()).intersection(set(paways_mirna_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cde7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "paways_genes_dicts ={}\n",
    "paways_mirna_dicts ={}\n",
    "\n",
    "for i in union_kegg:\n",
    "    paways_genes_dicts[i] = paways_genes_dict[i]\n",
    "    \n",
    "for i in union_kegg:\n",
    "    paways_mirna_dicts[i] = paways_mirna_dict[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fe7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genes_existed_pathway = []\n",
    "\n",
    "mirna_existed_pathway = []\n",
    "\n",
    "for index in paways_genes_dicts.keys():\n",
    "    genes_existed_pathway = genes_existed_pathway+ list(paways_genes_dicts[index])\n",
    "genes_existed_pathway = set(genes_existed_pathway)\n",
    "\n",
    "\n",
    "for index in paways_mirna_dicts.keys():\n",
    "    mirna_existed_pathway = mirna_existed_pathway+ list(paways_mirna_dicts[index])\n",
    "mirna_existed_pathway = set(mirna_existed_pathway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8c874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7768\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "print(len(genes_existed_pathway))\n",
    "print(len(mirna_existed_pathway))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948c50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3877bbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "miRNA_data = pd.read_csv(\"./AML_data/miRNA_data.csv\",index_col = 0)\n",
    "\n",
    "mRNA_data = pd.read_csv(\"./AML_data/mRNA_data.csv\",index_col = 0)\n",
    "\n",
    "example_case = pd.read_csv('./AML_data/response.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8a004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 2000)\n",
      "(221, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mRNA_data.shape)\n",
    "print(miRNA_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165fe600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRNA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1967e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5295fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_gene_miRNA = list(miRNA_data.columns)\n",
    "union_gene_mRNA = list(mRNA_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b65eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_union = list(paways_genes_dicts.keys())\n",
    "len(pathway_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0608bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = [union_gene_mRNA]\n",
    "#包含各类组学数据的特征名称列表，这里是union_gene_mRNA，表示mRNA组学的基因名称列表\n",
    "gene_pathway_bp_dfs = []\n",
    "#用于存储构建的关系矩阵的列表\n",
    "\n",
    "for i in range(len(mask_list)):\n",
    "    pathways_genes = np.zeros((len(pathway_union), len(mask_list[i]))) \n",
    "    #初始化的全零矩阵，行数为通路数量，列数为基因数量\n",
    "\n",
    "    for p  in pathway_union:\n",
    "        gs = paways_genes_dicts[p]\n",
    "        #paways_genes_dicts：字典数据结构，键是通路ID，值是属于该通路的基因列表\n",
    "        #gs：当前通路p对应的基因列表\n",
    "        g_inds = [mask_list[i].index(g) for g in gs if g in mask_list[i]]\n",
    "        #g_inds：当前通路p对应的基因在mask_list[i]中的索引列表\n",
    "        p_ind = pathway_union.index(p)\n",
    "        #p_ind：当前通路p在pathway_union中的索引\n",
    "        pathways_genes[p_ind, g_inds] = 1\n",
    "    gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=mask_list[i])\n",
    "    #最终构建的关系矩阵，行是通路，列是基因，值为1表示该基因属于该通路\n",
    "    \n",
    "    gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "#这里gene_pathway_bp_dfs是基因-通路矩阵,也就是先验知识\n",
    "    \n",
    "\n",
    "pathways_genes = np.zeros((len(pathway_union), len(union_gene_miRNA))) \n",
    "#初始化全零矩阵，行数为通路数量，列数为miRNA数量\n",
    "for p  in pathway_union:\n",
    "    gs = paways_mirna_dicts[p]\n",
    "    #paways_mirna_dicts：字典数据结构，键是通路ID，值是属于该通路的miRNA列表\n",
    "    #gs：当前通路p对应的miRNA列表\n",
    "    g_inds = [union_gene_miRNA.index(g) for g in gs if g in union_gene_miRNA]\n",
    "    #g_inds：当前通路p对应的miRNA在union_gene_miRNA中的索引列表\n",
    "    p_ind = pathway_union.index(p)\n",
    "    pathways_genes[p_ind, g_inds] = 1\n",
    "gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=union_gene_miRNA)\n",
    "#最终构建的关系矩阵，行是通路，列是miRNA，值为1表示该miRNA属于该通路\n",
    "gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a9d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gene_pathway_bp_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15821250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_pathway_bp_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125e5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import glorot_uniform, Initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D,Layer\n",
    "from tensorflow.keras import initializers,activations,regularizers\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    " \n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biological_module(Layer):\n",
    "    def __init__(self, units, mapp=None, nonzero_ind=None, kernel_initializer='glorot_uniform', W_regularizer=None,\n",
    "                 activation='tanh', use_bias=True,bias_initializer='zeros', bias_regularizer=None,\n",
    "                 bias_constraint=None,**kwargs):\n",
    "        \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.mapp = mapp\n",
    "        self.nonzero_ind = nonzero_ind\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activation_fn = activations.get(activation)\n",
    "        super(Biological_module, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        input_dim = input_shape[1]\n",
    "   \n",
    "\n",
    "        if not self.mapp is None:\n",
    "            self.mapp = self.mapp.astype(np.float32)\n",
    "\n",
    "   \n",
    "        if self.nonzero_ind is None:\n",
    "            nonzero_ind = np.array(np.nonzero(self.mapp)).T\n",
    "            self.nonzero_ind = nonzero_ind\n",
    "\n",
    "        self.kernel_shape = (input_dim, self.units)\n",
    "        \n",
    "\n",
    "        nonzero_count = self.nonzero_ind.shape[0]   \n",
    "\n",
    "\n",
    "        self.kernel_vector = self.add_weight(name='kernel_vector',\n",
    "                                             shape=(nonzero_count,),\n",
    "                                             initializer=self.kernel_initializer,\n",
    "                                             regularizer=self.kernel_regularizer,\n",
    "                                             trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer\n",
    "                                        )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        super(Biological_module, self).build(input_shape)  \n",
    "      \n",
    "\n",
    "    def call(self, inputs):# 前向传播\n",
    "        \n",
    "        ## 从非零索引重建关系矩阵\n",
    "        trans = tf.scatter_nd(tf.constant(self.nonzero_ind, tf.int32), self.kernel_vector,\n",
    "                           tf.constant(list(self.kernel_shape)))\n",
    "        ## 实现公式2中的矩阵乘法 X^z * M^z\n",
    "        output = K.dot(inputs, trans)\n",
    "        \n",
    "        ## 如果使用偏置，则添加偏置\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "            \n",
    "        ## 如果激活函数不为空，则应用激活函数\n",
    "        if self.activation_fn is not None:\n",
    "            output = self.activation_fn(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': self.activation,\n",
    "            'use_bias': self.use_bias,\n",
    "            'nonzero_ind': np.array(self.nonzero_ind),\n",
    "          \n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'W_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "\n",
    "        }\n",
    "        base_config = super(Biological_module, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "      \n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c438cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    " \n",
    "    def __init__(self, output_dim,  W_regularizer=None,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      trainable=True)\n",
    " \n",
    "        super(Self_Attention, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        #self.kernel是一个形状为(3, input_shape[1], output_dim)\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    " \n",
    "\n",
    "        print(\"K.permute_dimensions(WK.shape\",(K.permute_dimensions(WK,[1,0]).shape))\n",
    " \n",
    "        # 计算注意力分数\n",
    "        QK =  K.dot(K.permute_dimensions(WK,[1,0]),WQ)\n",
    "    \n",
    "        ## 缩放除以sqrt(d_k)，对应公式(3)中的根号d_k\n",
    "        QK = QK / (64**0.5)\n",
    " \n",
    "        QK = K.softmax(QK)\n",
    " \n",
    "        print(\"QK.shape\",QK.shape) ## 这里的QK就是公式3中的α_ij矩阵\n",
    " \n",
    "        # 利用注意力分数对值矩阵进行加权\n",
    "        V = K.dot(WV, QK)  # 将注意力分数应用到值矩阵上\n",
    "        \n",
    "        print(V.shape)\n",
    " \n",
    "        return V\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          \n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "\n",
    "        }\n",
    "        base_config = super(Self_Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    " \n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab00da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "020e30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mRNA_data,miRNA_data):\n",
    "    \n",
    "\n",
    "    \n",
    "    S_inputs_mRNA = Input(shape=(mRNA_data.shape[1],), dtype='float32',name= 'mRNA_inputs')\n",
    "  \n",
    "    S_inputs_miRNA = Input(shape=(miRNA_data.shape[1],), dtype='float32',name= 'miRNA_inputs')\n",
    "    \n",
    "\n",
    "   \n",
    "    h0_mRNA = Biological_module(gene_pathway_bp_dfs[0].shape[0],mapp =gene_pathway_bp_dfs[0].values.T, name = 'h0_mRNA',W_regularizer=l2(0.001))(S_inputs_mRNA)\n",
    "    #W_regularizer=l2(0.001)就是正则化项\n",
    "    \n",
    "    h0_miRNA = Biological_module(gene_pathway_bp_dfs[1].shape[0],mapp =gene_pathway_bp_dfs[1].values.T, name = 'h0_miRNA',W_regularizer=l2(0.001))(S_inputs_miRNA)\n",
    "\n",
    "\n",
    "\n",
    "    atten1 = Self_Attention(64,W_regularizer=l2(0.003))(h0_mRNA)\n",
    "    atten2 = Self_Attention(64,W_regularizer=l2(0.003))(h0_miRNA)\n",
    "    \n",
    "    feature_tal = tf.keras.layers.concatenate([atten1,atten2])\n",
    "    #连接起来\n",
    "\n",
    "    #MLP\n",
    "    h4 = tf.keras.layers.Dense(32,activation='tanh')(feature_tal)\n",
    "    \n",
    "    h5 = tf.keras.layers.Dense(1,activation='sigmoid')(h4)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[S_inputs_mRNA,S_inputs_miRNA], outputs=h5)\n",
    "\n",
    "    model.summary()\n",
    "    # 使用Adam优化器，学习率0.0001，衰减0.0001\n",
    "    # 使用二元交叉熵损失函数，评估指标为准确率\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.0001,decay=0.0001) \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1044dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "   \n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def get_metrics(true_score,pre_score,pre_probe):\n",
    "    \n",
    "  \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_score, pre_probe, pos_label=1)\n",
    "   \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    aupr = average_precision_score(true_score, pre_probe)\n",
    "    \n",
    "    pre, rec, thresholds = precision_recall_curve(true_score, pre_probe)    \n",
    "    auprc  = metrics.auc(rec, pre)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(true_score,pre_score)\n",
    "    \n",
    "    f1 = metrics.f1_score(true_score, pre_score)\n",
    "    \n",
    "    precision = metrics.precision_score(true_score,pre_score)\n",
    "    \n",
    "    recall = metrics.recall_score(true_score,pre_score)\n",
    "    \n",
    "     \n",
    "    print( print(confusion_matrix(true_score,pre_score)))\n",
    "    return precision,accuracy,recall,f1,auc,aupr,auprc\n",
    "\n",
    "\n",
    "def evaluates(y_test, y_pred):\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    \n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)    \n",
    "    auprc  = metrics.auc(recall, precision)\n",
    "    \n",
    "    pp = [1 if index>=0.5  else 0 for index in  y_pred ]\n",
    "    \n",
    "    pre = metrics.precision_score(y_test,pp)\n",
    "    \n",
    "    f1 = metrics.f1_score(y_test,pp)\n",
    "    \n",
    "    rec = metrics.recall_score(y_test,pp)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test,pp)\n",
    "    \n",
    "    print(confusion_matrix(y_test,pp))\n",
    "    return pre,acc,rec,f1,auc,aupr,auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cad9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 120 101\n",
      "1.0940594059405941 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "y = example_case['response'].values\n",
    "n_samples =example_case['response'].values\n",
    "\n",
    "print(len(n_samples),n_samples.sum(),(len(n_samples) -n_samples.sum()))\n",
    "\n",
    "x_0 =  len(n_samples) / (2*  (len(n_samples) -n_samples.sum()))\n",
    "x_1 =  len(n_samples) / (2*  n_samples.sum())\n",
    "\n",
    "print(x_0,x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80621e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\10263\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention (Self_Attention (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_1 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           self__attention[0][0]            \n",
      "                                                                 self__attention_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           4128        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 176 samples\n",
      "Epoch 1/130\n",
      "176/176 [==============================] - 0s 903us/sample - loss: 0.9219 - acc: 0.4773\n",
      "Epoch 2/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.9178 - acc: 0.5455\n",
      "Epoch 3/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.9137 - acc: 0.5455\n",
      "Epoch 4/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.9096 - acc: 0.5455\n",
      "Epoch 5/130\n",
      "176/176 [==============================] - 0s 105us/sample - loss: 0.9056 - acc: 0.5455\n",
      "Epoch 6/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.9016 - acc: 0.5455\n",
      "Epoch 7/130\n",
      "176/176 [==============================] - 0s 87us/sample - loss: 0.8977 - acc: 0.5455\n",
      "Epoch 8/130\n",
      "176/176 [==============================] - 0s 73us/sample - loss: 0.8938 - acc: 0.5455\n",
      "Epoch 9/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8898 - acc: 0.5455\n",
      "Epoch 10/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.8859 - acc: 0.5455\n",
      "Epoch 11/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.8821 - acc: 0.5455\n",
      "Epoch 12/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.8782 - acc: 0.5455\n",
      "Epoch 13/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.8743 - acc: 0.5682\n",
      "Epoch 14/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8705 - acc: 0.6136\n",
      "Epoch 15/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8666 - acc: 0.6420\n",
      "Epoch 16/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8627 - acc: 0.6705\n",
      "Epoch 17/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8587 - acc: 0.6875\n",
      "Epoch 18/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.8546 - acc: 0.6989\n",
      "Epoch 19/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8504 - acc: 0.6989\n",
      "Epoch 20/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8460 - acc: 0.6761\n",
      "Epoch 21/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8416 - acc: 0.6932\n",
      "Epoch 22/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8369 - acc: 0.6989\n",
      "Epoch 23/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8318 - acc: 0.7045\n",
      "Epoch 24/130\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.8356 - acc: 0.703 - 0s 74us/sample - loss: 0.8264 - acc: 0.7273\n",
      "Epoch 25/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.8209 - acc: 0.7273\n",
      "Epoch 26/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8152 - acc: 0.7159\n",
      "Epoch 27/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8086 - acc: 0.7216\n",
      "Epoch 28/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.8022 - acc: 0.7045\n",
      "Epoch 29/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.7951 - acc: 0.7216\n",
      "Epoch 30/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7873 - acc: 0.7273\n",
      "Epoch 31/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.7794 - acc: 0.7443\n",
      "Epoch 32/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.7712 - acc: 0.7386\n",
      "Epoch 33/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7625 - acc: 0.7443\n",
      "Epoch 34/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.7530 - acc: 0.7500\n",
      "Epoch 35/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.7436 - acc: 0.7557\n",
      "Epoch 36/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.7337 - acc: 0.7500\n",
      "Epoch 37/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.7237 - acc: 0.7500\n",
      "Epoch 38/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.7138 - acc: 0.7557\n",
      "Epoch 39/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.7029 - acc: 0.7500\n",
      "Epoch 40/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 41/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.6847 - acc: 0.7386\n",
      "Epoch 42/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.6749 - acc: 0.7443\n",
      "Epoch 43/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.6663 - acc: 0.7500\n",
      "Epoch 44/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.6590 - acc: 0.7614\n",
      "Epoch 45/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.6512 - acc: 0.7614\n",
      "Epoch 46/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.6437 - acc: 0.7614\n",
      "Epoch 47/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.6372 - acc: 0.7614\n",
      "Epoch 48/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.6303 - acc: 0.7614\n",
      "Epoch 49/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.6250 - acc: 0.7500\n",
      "Epoch 50/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.6194 - acc: 0.7443\n",
      "Epoch 51/130\n",
      "176/176 [==============================] - 0s 71us/sample - loss: 0.6137 - acc: 0.7557\n",
      "Epoch 52/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.6085 - acc: 0.7557\n",
      "Epoch 53/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.6036 - acc: 0.7614\n",
      "Epoch 54/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.5982 - acc: 0.7614\n",
      "Epoch 55/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5937 - acc: 0.7670\n",
      "Epoch 56/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.5895 - acc: 0.7670\n",
      "Epoch 57/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.5858 - acc: 0.7727\n",
      "Epoch 58/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5809 - acc: 0.7670\n",
      "Epoch 59/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5755 - acc: 0.7614\n",
      "Epoch 60/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5715 - acc: 0.7727\n",
      "Epoch 61/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5679 - acc: 0.7727\n",
      "Epoch 62/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5637 - acc: 0.7727\n",
      "Epoch 63/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5600 - acc: 0.7727\n",
      "Epoch 64/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.5548 - acc: 0.7784\n",
      "Epoch 65/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5510 - acc: 0.7784\n",
      "Epoch 66/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.5475 - acc: 0.7784\n",
      "Epoch 67/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5433 - acc: 0.7841\n",
      "Epoch 68/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.5393 - acc: 0.7784\n",
      "Epoch 69/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5349 - acc: 0.7841\n",
      "Epoch 70/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5313 - acc: 0.7784\n",
      "Epoch 71/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.5279 - acc: 0.7898\n",
      "Epoch 72/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5238 - acc: 0.7898\n",
      "Epoch 73/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5207 - acc: 0.7898\n",
      "Epoch 74/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5158 - acc: 0.7898\n",
      "Epoch 75/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5125 - acc: 0.7898\n",
      "Epoch 76/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5084 - acc: 0.8011\n",
      "Epoch 77/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.5043 - acc: 0.7955\n",
      "Epoch 78/130\n",
      "176/176 [==============================] - 0s 64us/sample - loss: 0.5025 - acc: 0.7898\n",
      "Epoch 79/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.4978 - acc: 0.7955\n",
      "Epoch 80/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.4937 - acc: 0.8011\n",
      "Epoch 81/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4906 - acc: 0.8011\n",
      "Epoch 82/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4867 - acc: 0.7955\n",
      "Epoch 83/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4835 - acc: 0.8068\n",
      "Epoch 84/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.4802 - acc: 0.7955\n",
      "Epoch 85/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.4758 - acc: 0.8068\n",
      "Epoch 86/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.4724 - acc: 0.8068\n",
      "Epoch 87/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.4697 - acc: 0.8068\n",
      "Epoch 88/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.4661 - acc: 0.8068\n",
      "Epoch 89/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4629 - acc: 0.8068\n",
      "Epoch 90/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4593 - acc: 0.8239\n",
      "Epoch 91/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.4561 - acc: 0.8239\n",
      "Epoch 92/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.4533 - acc: 0.8239\n",
      "Epoch 93/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4491 - acc: 0.8239\n",
      "Epoch 94/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4459 - acc: 0.8182\n",
      "Epoch 95/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4430 - acc: 0.8182\n",
      "Epoch 96/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.4400 - acc: 0.8182\n",
      "Epoch 97/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.4370 - acc: 0.8182\n",
      "Epoch 98/130\n",
      "176/176 [==============================] - 0s 127us/sample - loss: 0.4337 - acc: 0.8295\n",
      "Epoch 99/130\n",
      "176/176 [==============================] - 0s 118us/sample - loss: 0.4307 - acc: 0.8295\n",
      "Epoch 100/130\n",
      "176/176 [==============================] - 0s 102us/sample - loss: 0.4279 - acc: 0.8295\n",
      "Epoch 101/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.4261 - acc: 0.8295\n",
      "Epoch 102/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.4216 - acc: 0.8295\n",
      "Epoch 103/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.4185 - acc: 0.8295\n",
      "Epoch 104/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4157 - acc: 0.8352\n",
      "Epoch 105/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.4129 - acc: 0.8352\n",
      "Epoch 106/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4101 - acc: 0.8352\n",
      "Epoch 107/130\n",
      "176/176 [==============================] - 0s 63us/sample - loss: 0.4078 - acc: 0.8352\n",
      "Epoch 108/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4045 - acc: 0.8352\n",
      "Epoch 109/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4011 - acc: 0.8352\n",
      "Epoch 110/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.3984 - acc: 0.8352\n",
      "Epoch 111/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3958 - acc: 0.8352\n",
      "Epoch 112/130\n",
      "176/176 [==============================] - 0s 92us/sample - loss: 0.3933 - acc: 0.8352\n",
      "Epoch 113/130\n",
      "176/176 [==============================] - 0s 89us/sample - loss: 0.3897 - acc: 0.8352\n",
      "Epoch 114/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3879 - acc: 0.8352\n",
      "Epoch 115/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.3847 - acc: 0.8409\n",
      "Epoch 116/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3818 - acc: 0.8409\n",
      "Epoch 117/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3790 - acc: 0.8409\n",
      "Epoch 118/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.3763 - acc: 0.8409\n",
      "Epoch 119/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3737 - acc: 0.8636\n",
      "Epoch 120/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3716 - acc: 0.8580\n",
      "Epoch 121/130\n",
      "176/176 [==============================] - 0s 89us/sample - loss: 0.3678 - acc: 0.8580\n",
      "Epoch 122/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.3655 - acc: 0.8693\n",
      "Epoch 123/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.3627 - acc: 0.8693\n",
      "Epoch 124/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.3604 - acc: 0.8636\n",
      "Epoch 125/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.3578 - acc: 0.8693\n",
      "Epoch 126/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3545 - acc: 0.8693\n",
      "Epoch 127/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.3523 - acc: 0.8750\n",
      "Epoch 128/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.3498 - acc: 0.8750\n",
      "Epoch 129/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.3478 - acc: 0.8636\n",
      "Epoch 130/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.3443 - acc: 0.8750\n",
      "[[17  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8492063492063493, 0.8922863501859651, 0.8901487708545767)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_2 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_3 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           self__attention_2[0][0]          \n",
      "                                                                 self__attention_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 608us/sample - loss: 0.9219 - acc: 0.4689\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.9178 - acc: 0.5480\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.9137 - acc: 0.6215\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9097 - acc: 0.6497\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.9057 - acc: 0.6554\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.9017 - acc: 0.6893\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8977 - acc: 0.6949\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8938 - acc: 0.7175\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8900 - acc: 0.7006\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.8861 - acc: 0.6836\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8823 - acc: 0.6893\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8784 - acc: 0.7345\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8746 - acc: 0.7458\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8708 - acc: 0.7458\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.8670 - acc: 0.7458\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.8631 - acc: 0.7401\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8592 - acc: 0.7401\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8552 - acc: 0.7401\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8513 - acc: 0.7458\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8471 - acc: 0.7401\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8429 - acc: 0.7571\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8385 - acc: 0.7627\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8340 - acc: 0.7571\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8291 - acc: 0.7571\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8240 - acc: 0.7571\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8189 - acc: 0.7571\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8130 - acc: 0.7514\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8072 - acc: 0.7458\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8006 - acc: 0.7345\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7935 - acc: 0.7288\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7862 - acc: 0.7288\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7790 - acc: 0.7288\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7711 - acc: 0.7345\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7624 - acc: 0.7288\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7539 - acc: 0.7288\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.7451 - acc: 0.7345\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.7350 - acc: 0.7458\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7256 - acc: 0.7345\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.7159 - acc: 0.7514\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7066 - acc: 0.7514\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6973 - acc: 0.7514\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6873 - acc: 0.7571\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6784 - acc: 0.7627\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.6694 - acc: 0.7684\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.6607 - acc: 0.7627\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6528 - acc: 0.7514\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6433 - acc: 0.7571\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6357 - acc: 0.7571\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6309 - acc: 0.7740\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.6115 - acc: 0.796 - 0s 81us/sample - loss: 0.6222 - acc: 0.7740\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.6153 - acc: 0.7797\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6094 - acc: 0.7684\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.6039 - acc: 0.7740\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.5974 - acc: 0.7684\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.5919 - acc: 0.7797\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 103us/sample - loss: 0.5864 - acc: 0.7797\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5812 - acc: 0.7853\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5757 - acc: 0.7853\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5701 - acc: 0.7853\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5652 - acc: 0.7853\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.5600 - acc: 0.7853\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5550 - acc: 0.7910\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5514 - acc: 0.7910\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5455 - acc: 0.7966\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5407 - acc: 0.7966\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5360 - acc: 0.8079\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5322 - acc: 0.8023\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5279 - acc: 0.7910\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5227 - acc: 0.7966\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5178 - acc: 0.8136\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5148 - acc: 0.8192\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5101 - acc: 0.8192\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.5054 - acc: 0.8136\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.5017 - acc: 0.8136\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4976 - acc: 0.8192\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4934 - acc: 0.8192\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4900 - acc: 0.8192\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4854 - acc: 0.8305\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4816 - acc: 0.8305\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.4778 - acc: 0.8305\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4746 - acc: 0.8305\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4700 - acc: 0.8305\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4663 - acc: 0.8305\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4629 - acc: 0.8418\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4598 - acc: 0.8418\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4563 - acc: 0.8418\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4522 - acc: 0.8418\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4489 - acc: 0.8475\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4454 - acc: 0.8531\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4422 - acc: 0.8531\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4384 - acc: 0.8588\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4364 - acc: 0.8588\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4322 - acc: 0.8588\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4292 - acc: 0.8531\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4274 - acc: 0.8531\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4236 - acc: 0.8588\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.4202 - acc: 0.8531\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4166 - acc: 0.8588\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4138 - acc: 0.8588\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4105 - acc: 0.8644\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4078 - acc: 0.8588\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4068 - acc: 0.8644\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4013 - acc: 0.8701\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4009 - acc: 0.8531\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3974 - acc: 0.8701\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.3938 - acc: 0.8644\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3908 - acc: 0.8644\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3880 - acc: 0.8757\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.3854 - acc: 0.8757\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3825 - acc: 0.8701\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.3798 - acc: 0.8757\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 125us/sample - loss: 0.3768 - acc: 0.8757\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3746 - acc: 0.8757\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3715 - acc: 0.8814\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3696 - acc: 0.8814\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3663 - acc: 0.8870\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3637 - acc: 0.8927\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3612 - acc: 0.8927\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3585 - acc: 0.8870\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3558 - acc: 0.8927\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3533 - acc: 0.8983\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3517 - acc: 0.8983\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3487 - acc: 0.8983\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3458 - acc: 0.8983\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3444 - acc: 0.8983\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3405 - acc: 0.8983\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3380 - acc: 0.8927\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3358 - acc: 0.8927\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3333 - acc: 0.8927\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3304 - acc: 0.8983\n",
      "[[13  7]\n",
      " [ 7 17]]\n",
      "None\n",
      "(0.7083333333333334, 0.6818181818181818, 0.7083333333333334, 0.7083333333333334, 0.7833333333333333, 0.8128524084692106, 0.8071086129427654)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_4 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_5 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           self__attention_4[0][0]          \n",
      "                                                                 self__attention_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           4128        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            33          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 583us/sample - loss: 0.9230 - acc: 0.4915\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.9189 - acc: 0.5537\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.9147 - acc: 0.5537\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.9106 - acc: 0.5593\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.9066 - acc: 0.5593\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.9026 - acc: 0.5650\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.8987 - acc: 0.5706\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.8948 - acc: 0.5650\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8909 - acc: 0.5706\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8871 - acc: 0.5650\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8833 - acc: 0.5650\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8796 - acc: 0.5650\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8759 - acc: 0.5706\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.8722 - acc: 0.5706\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.8685 - acc: 0.5706\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.8647 - acc: 0.5650\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 100us/sample - loss: 0.8611 - acc: 0.5650\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8575 - acc: 0.5650\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8537 - acc: 0.5650\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.8499 - acc: 0.5650\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8462 - acc: 0.5650\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8423 - acc: 0.5706\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.8383 - acc: 0.5706\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8341 - acc: 0.5706\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.8301 - acc: 0.5763\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8256 - acc: 0.5932\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8211 - acc: 0.5989\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8168 - acc: 0.5989\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8122 - acc: 0.6045\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.8076 - acc: 0.5932\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8022 - acc: 0.6158\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.7968 - acc: 0.6441\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.7910 - acc: 0.6497\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7848 - acc: 0.6667\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7780 - acc: 0.6836\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7711 - acc: 0.7119\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.7633 - acc: 0.7175\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.7558 - acc: 0.7401\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7477 - acc: 0.7288\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7390 - acc: 0.7401\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7311 - acc: 0.7401\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7223 - acc: 0.7345\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7149 - acc: 0.7345\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 128us/sample - loss: 0.7064 - acc: 0.7401\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.6986 - acc: 0.7401\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.6909 - acc: 0.7401\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6835 - acc: 0.7401\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6755 - acc: 0.7458\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.6692 - acc: 0.7288\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6622 - acc: 0.7514\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.6555 - acc: 0.7458\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6484 - acc: 0.7514\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.6431 - acc: 0.7571\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6368 - acc: 0.7571\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.6317 - acc: 0.7571\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6261 - acc: 0.7627\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6202 - acc: 0.7514\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6175 - acc: 0.7401\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6119 - acc: 0.7401\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6068 - acc: 0.7571\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.6016 - acc: 0.7571\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.5973 - acc: 0.7627\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5927 - acc: 0.7627\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5890 - acc: 0.7571\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5841 - acc: 0.7627\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5797 - acc: 0.7684\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5761 - acc: 0.7627\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5709 - acc: 0.7627\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5671 - acc: 0.7627\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5625 - acc: 0.7627\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5580 - acc: 0.7627\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5546 - acc: 0.7627\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.5500 - acc: 0.7627\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5458 - acc: 0.7627\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5417 - acc: 0.7627\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5375 - acc: 0.7684\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5337 - acc: 0.7797\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5308 - acc: 0.7740\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.5257 - acc: 0.7853\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5208 - acc: 0.7910\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5166 - acc: 0.7910\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5141 - acc: 0.7853\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.5088 - acc: 0.8023\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5051 - acc: 0.7966\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5021 - acc: 0.7966\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4971 - acc: 0.8079\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4930 - acc: 0.8136\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4894 - acc: 0.8136\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4862 - acc: 0.8136\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4822 - acc: 0.8136\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.4782 - acc: 0.8136\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4739 - acc: 0.8305\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4703 - acc: 0.8418\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4671 - acc: 0.8475\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4631 - acc: 0.8475\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4597 - acc: 0.8475\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4554 - acc: 0.8475\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4525 - acc: 0.8418\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4485 - acc: 0.8418\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4448 - acc: 0.8475\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.4413 - acc: 0.8475\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.4387 - acc: 0.8475\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4344 - acc: 0.8475\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4306 - acc: 0.8475\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4277 - acc: 0.8418\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4245 - acc: 0.8418\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.4213 - acc: 0.8418\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4175 - acc: 0.8475\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4157 - acc: 0.8531\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4125 - acc: 0.8418\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4076 - acc: 0.8475\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4042 - acc: 0.8531\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.4017 - acc: 0.8475\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3983 - acc: 0.8644\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3949 - acc: 0.8701\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3926 - acc: 0.8644\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3897 - acc: 0.8644\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3864 - acc: 0.8644\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.3832 - acc: 0.8701\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3799 - acc: 0.8701\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3773 - acc: 0.8701\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3736 - acc: 0.8757\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3708 - acc: 0.8757\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3683 - acc: 0.8814\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3662 - acc: 0.8757\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3623 - acc: 0.8814\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3595 - acc: 0.8870\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3570 - acc: 0.8870\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3540 - acc: 0.8870\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3512 - acc: 0.8927\n",
      "[[16  4]\n",
      " [ 5 19]]\n",
      "None\n",
      "(0.8260869565217391, 0.7954545454545454, 0.7916666666666666, 0.8085106382978724, 0.89375, 0.9072716160008149, 0.904799986691756)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_6 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_7 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           self__attention_6[0][0]          \n",
      "                                                                 self__attention_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           4128        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 749us/sample - loss: 0.9210 - acc: 0.5424\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.9169 - acc: 0.5424\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.9127 - acc: 0.5424\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.9087 - acc: 0.5424\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9046 - acc: 0.5537\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.9006 - acc: 0.5593\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8965 - acc: 0.5932\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.8925 - acc: 0.6441\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8885 - acc: 0.6610\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8845 - acc: 0.6723\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.8805 - acc: 0.6780\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8764 - acc: 0.6893\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.8724 - acc: 0.6893\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 177us/sample - loss: 0.8683 - acc: 0.7006\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.8643 - acc: 0.7006\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 119us/sample - loss: 0.8602 - acc: 0.6893\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.8559 - acc: 0.6780\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.8516 - acc: 0.7062\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8473 - acc: 0.7288\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8429 - acc: 0.7175\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.8382 - acc: 0.7232\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8333 - acc: 0.7232\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8282 - acc: 0.7288\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8228 - acc: 0.7345\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.8171 - acc: 0.7175\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.8111 - acc: 0.7119\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8046 - acc: 0.7062\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7979 - acc: 0.7175\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7907 - acc: 0.7175\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7828 - acc: 0.7175\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.7750 - acc: 0.7232\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7676 - acc: 0.7175\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7594 - acc: 0.7062\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.7501 - acc: 0.7062\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7418 - acc: 0.7175\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7338 - acc: 0.7232\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7244 - acc: 0.7288\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7166 - acc: 0.7232\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.7079 - acc: 0.7288\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6997 - acc: 0.7232\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6914 - acc: 0.7232\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6836 - acc: 0.7232\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.6766 - acc: 0.7288\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.6699 - acc: 0.7345\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6618 - acc: 0.7288\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.6548 - acc: 0.7345\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.6502 - acc: 0.7345\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6428 - acc: 0.7401\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.6366 - acc: 0.7345\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.6316 - acc: 0.7458\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6252 - acc: 0.7571\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.6219 - acc: 0.7514\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.6146 - acc: 0.7514\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.6091 - acc: 0.7684\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6037 - acc: 0.7684\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 131us/sample - loss: 0.5988 - acc: 0.7684\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.5937 - acc: 0.7684\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5887 - acc: 0.7740\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 117us/sample - loss: 0.5838 - acc: 0.7627\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5782 - acc: 0.7627\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5739 - acc: 0.7571\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5684 - acc: 0.7684\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5633 - acc: 0.7627\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5593 - acc: 0.7684\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5539 - acc: 0.7740\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5484 - acc: 0.7797\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5444 - acc: 0.7853\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5394 - acc: 0.7797\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5342 - acc: 0.7966\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5314 - acc: 0.7910\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.5249 - acc: 0.7966\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5208 - acc: 0.7966\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5160 - acc: 0.7966\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5118 - acc: 0.8023\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5071 - acc: 0.8136\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5026 - acc: 0.8192\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4981 - acc: 0.8192\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4942 - acc: 0.8192\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4901 - acc: 0.8192\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.4856 - acc: 0.8192\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4813 - acc: 0.8249\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4781 - acc: 0.8362\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4734 - acc: 0.8249\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4703 - acc: 0.8249\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.4660 - acc: 0.8362\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4616 - acc: 0.8418\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.4590 - acc: 0.8249\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.4546 - acc: 0.8362\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4506 - acc: 0.8418\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4467 - acc: 0.8531\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4434 - acc: 0.8475\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4407 - acc: 0.8475\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4358 - acc: 0.8588\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.4345 - acc: 0.8644\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4298 - acc: 0.8644\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4254 - acc: 0.8757\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4221 - acc: 0.8588\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 108us/sample - loss: 0.4199 - acc: 0.8701\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.4189 - acc: 0.8701\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4124 - acc: 0.8814\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4092 - acc: 0.8814\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4049 - acc: 0.8814\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.4032 - acc: 0.8814\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4001 - acc: 0.8814\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.3971 - acc: 0.8927\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3936 - acc: 0.8870\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3913 - acc: 0.8814\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.3878 - acc: 0.8870\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3833 - acc: 0.8814\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.3829 - acc: 0.8870\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3788 - acc: 0.8927\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3760 - acc: 0.8814\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3721 - acc: 0.8870\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.3692 - acc: 0.8870\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3665 - acc: 0.8870\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3632 - acc: 0.8927\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3604 - acc: 0.8927\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3578 - acc: 0.8927\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3548 - acc: 0.8870\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3533 - acc: 0.8927\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3496 - acc: 0.8927\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.3476 - acc: 0.9040\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.3438 - acc: 0.8814\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3414 - acc: 0.8870\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3395 - acc: 0.8870\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3360 - acc: 0.8927\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3334 - acc: 0.8983\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.3315 - acc: 0.9153\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3285 - acc: 0.9209\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3260 - acc: 0.9096\n",
      "[[16  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8181818181818182, 0.8333333333333334, 0.8333333333333334, 0.9041666666666667, 0.9286115411638358, 0.927137765276254)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_8 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_9 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           self__attention_8[0][0]          \n",
      "                                                                 self__attention_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           4128        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            33          dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 733us/sample - loss: 0.9233 - acc: 0.4576\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9189 - acc: 0.4576\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9147 - acc: 0.4576\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.9105 - acc: 0.5085\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 150us/sample - loss: 0.9063 - acc: 0.5989\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.9021 - acc: 0.6723\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 125us/sample - loss: 0.8979 - acc: 0.7175\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8938 - acc: 0.7458\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8896 - acc: 0.7514\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.8855 - acc: 0.7571\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8813 - acc: 0.7345\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8772 - acc: 0.7175\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8729 - acc: 0.7232\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8687 - acc: 0.7288\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.8643 - acc: 0.7345\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.8600 - acc: 0.7232\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.8554 - acc: 0.7288\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.8508 - acc: 0.7288\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 100us/sample - loss: 0.8459 - acc: 0.7345\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8410 - acc: 0.7175\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8355 - acc: 0.7232\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8298 - acc: 0.7345\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8239 - acc: 0.7232\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.8177 - acc: 0.7175\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8110 - acc: 0.7232\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8038 - acc: 0.7232\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.7967 - acc: 0.7401\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.7882 - acc: 0.7401\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.7799 - acc: 0.7458\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.7714 - acc: 0.7401\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.7625 - acc: 0.7401\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.7525 - acc: 0.7401\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.7431 - acc: 0.7401\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7340 - acc: 0.7401\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7238 - acc: 0.7458\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.7145 - acc: 0.7401\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.7053 - acc: 0.7345\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.6968 - acc: 0.7401\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 115us/sample - loss: 0.6869 - acc: 0.7401\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.6786 - acc: 0.7458\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 106us/sample - loss: 0.6707 - acc: 0.7458\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 101us/sample - loss: 0.6625 - acc: 0.7514\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6567 - acc: 0.7627\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.6489 - acc: 0.7514\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 115us/sample - loss: 0.6417 - acc: 0.7627\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.6350 - acc: 0.7627\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.6297 - acc: 0.7571\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.6234 - acc: 0.7684\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6175 - acc: 0.7684\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6120 - acc: 0.7627\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6063 - acc: 0.7684\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.6013 - acc: 0.7740\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5957 - acc: 0.7797\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.5917 - acc: 0.7740\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5861 - acc: 0.7740\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5808 - acc: 0.7740\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5758 - acc: 0.7797\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.5708 - acc: 0.7853\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5659 - acc: 0.7853\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5615 - acc: 0.7853\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5563 - acc: 0.7853\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.5514 - acc: 0.7966\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5479 - acc: 0.8023\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5430 - acc: 0.8079\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5393 - acc: 0.8079\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5333 - acc: 0.8079\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.5284 - acc: 0.8192\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5242 - acc: 0.8136\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5195 - acc: 0.8136\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.5152 - acc: 0.8136\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5105 - acc: 0.8136\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5065 - acc: 0.8136\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5021 - acc: 0.8192\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4975 - acc: 0.8136\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4933 - acc: 0.8136\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4893 - acc: 0.8249\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 136us/sample - loss: 0.4849 - acc: 0.8249\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.4806 - acc: 0.8362\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.4763 - acc: 0.8362\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4723 - acc: 0.8418\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.4684 - acc: 0.8418\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4658 - acc: 0.8475\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.4610 - acc: 0.8475\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4564 - acc: 0.8418\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4528 - acc: 0.8418\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4491 - acc: 0.8531\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4449 - acc: 0.8475\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4414 - acc: 0.8531\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4375 - acc: 0.8531\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.4336 - acc: 0.8531\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.4298 - acc: 0.8531\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.4264 - acc: 0.8531\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4227 - acc: 0.8531\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4195 - acc: 0.8531\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.4154 - acc: 0.8531\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.4121 - acc: 0.8531\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4111 - acc: 0.8644\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4053 - acc: 0.8701\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4011 - acc: 0.8701\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3979 - acc: 0.8870\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3965 - acc: 0.8757\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.3913 - acc: 0.8870\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3886 - acc: 0.8701\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3850 - acc: 0.8757\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3808 - acc: 0.8814\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3777 - acc: 0.8927\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.3754 - acc: 0.8870\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3723 - acc: 0.9096\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3672 - acc: 0.9040\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3647 - acc: 0.8983\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.3627 - acc: 0.8983\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 138us/sample - loss: 0.3591 - acc: 0.9096\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 120us/sample - loss: 0.3549 - acc: 0.9096\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3522 - acc: 0.9153\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3483 - acc: 0.9153\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3449 - acc: 0.9153\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3432 - acc: 0.9153\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3388 - acc: 0.9153\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.3353 - acc: 0.9209\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3333 - acc: 0.9209\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3304 - acc: 0.9266\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3264 - acc: 0.9209\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.3231 - acc: 0.9209\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3203 - acc: 0.9209\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.3176 - acc: 0.9322\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3142 - acc: 0.9322\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.3112 - acc: 0.9322\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 103us/sample - loss: 0.3087 - acc: 0.9322\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.3065 - acc: 0.9322\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3035 - acc: 0.9322\n",
      "[[13  7]\n",
      " [ 6 18]]\n",
      "None\n",
      "(0.72, 0.7045454545454546, 0.75, 0.7346938775510204, 0.8145833333333333, 0.8612621157756413, 0.8581736936595286)\n",
      "Cross validated results :  ACC = 0.7644444444444445, REC = 0.7833333333333334, F1 = 0.7836409031697786, AUC = 0.8490079365079366, AUPR =0.8804568063190936\n"
     ]
    }
   ],
   "source": [
    "#Five-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1030) \n",
    "\n",
    "kfscore = []\n",
    "p = 0\n",
    "for train_index, test_index in skf.split(mRNA_data.values,y):\n",
    "\n",
    "\n",
    "    mRNA_train_x = mRNA_data.values[train_index]\n",
    "    mRNA_test_x  = mRNA_data.values[test_index]\n",
    "\n",
    "    miRNA_train_x = miRNA_data.values[train_index]\n",
    "    miRNA_test_x  = miRNA_data.values[test_index]\n",
    "\n",
    "    train_y  = y[train_index]\n",
    "    test_y   = y[test_index]\n",
    "\n",
    "    model = create_model(mRNA_data,miRNA_data)\n",
    "    model.fit( { \"mRNA_inputs\": mRNA_train_x, 'miRNA_inputs':miRNA_train_x},train_y,\n",
    "                 epochs=130,batch_size = 64,class_weight = {0:x_0,1:x_1})  \n",
    "\n",
    "    y_pred = model.predict({\"mRNA_inputs\": mRNA_test_x,'miRNA_inputs':miRNA_test_x})\n",
    "\n",
    "    y_score = [1 if index>=0.5  else 0 for index in  y_pred]\n",
    "\n",
    "    evaluate_epoch = get_metrics(test_y,y_score,y_pred)\n",
    "    print(evaluate_epoch)\n",
    "\n",
    "    kfscore.append(evaluate_epoch)\n",
    "    \n",
    "results = list(np.array(kfscore).sum(axis= 0)/5.0)\n",
    "print('Cross validated results :  ACC = {}, REC = {}, F1 = {}, AUC = {}, AUPR ={}'.format(results[1],results[2],results[3],results[4],results[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58a98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e535d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepkngg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
