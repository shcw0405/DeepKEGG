{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094868a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ed8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes-pathways annotation\n",
    "\n",
    "path = './KEGG_pathways/20230205_kegg_hsa.gmt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_genes_dict = {}\n",
    "for i in files: \n",
    "    paways_genes_dict[i.split('\\t')[0].split('_')[0]] = i.replace('\\n','').split('\\t')[2:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-pathways annotation\n",
    "path = './KEGG_pathways/kegg_anano.txt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_mirna_dict = {}\n",
    "for i in files:\n",
    "     keys = i.split(',')[0].split('|')[1]\n",
    "     values1 = i.split(',')[1:-1]\n",
    "     values2 =  i.split(',')[-1].replace('\\n','')\n",
    "     values1.append(values2)\n",
    "     values1 =list(set(values1)) \n",
    "     paways_mirna_dict[keys] = values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27293b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_kegg = list(set(paways_genes_dict.keys()).intersection(set(paways_mirna_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cde7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "paways_genes_dicts ={}\n",
    "paways_mirna_dicts ={}\n",
    "\n",
    "for i in union_kegg:\n",
    "    paways_genes_dicts[i] = paways_genes_dict[i]\n",
    "    \n",
    "for i in union_kegg:\n",
    "    paways_mirna_dicts[i] = paways_mirna_dict[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fe7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genes_existed_pathway = []\n",
    "\n",
    "mirna_existed_pathway = []\n",
    "\n",
    "for index in paways_genes_dicts.keys():\n",
    "    genes_existed_pathway = genes_existed_pathway+ list(paways_genes_dicts[index])\n",
    "genes_existed_pathway = set(genes_existed_pathway)\n",
    "\n",
    "\n",
    "for index in paways_mirna_dicts.keys():\n",
    "    mirna_existed_pathway = mirna_existed_pathway+ list(paways_mirna_dicts[index])\n",
    "mirna_existed_pathway = set(mirna_existed_pathway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8c874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7768\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "print(len(genes_existed_pathway))\n",
    "print(len(mirna_existed_pathway))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948c50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3877bbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "miRNA_data = pd.read_csv(\"./AML_data/miRNA_data.csv\",index_col = 0)\n",
    "\n",
    "mRNA_data = pd.read_csv(\"./AML_data/mRNA_data.csv\",index_col = 0)\n",
    "\n",
    "example_case = pd.read_csv('./AML_data/response.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b8a004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 2000)\n",
      "(221, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mRNA_data.shape)\n",
    "print(miRNA_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165fe600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRNA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1967e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5295fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_gene_miRNA = list(miRNA_data.columns)\n",
    "union_gene_mRNA = list(mRNA_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b65eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_union = list(paways_genes_dicts.keys())\n",
    "len(pathway_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0608bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = [union_gene_mRNA]\n",
    "\n",
    "gene_pathway_bp_dfs = []\n",
    "\n",
    "\n",
    "for i in range(len(mask_list)):\n",
    "    pathways_genes = np.zeros((len(pathway_union), len(mask_list[i]))) \n",
    "    for p  in pathway_union:\n",
    "        gs = paways_genes_dicts[p]\n",
    "        g_inds = [mask_list[i].index(g) for g in gs if g in mask_list[i]]\n",
    "        p_ind = pathway_union.index(p)\n",
    "        pathways_genes[p_ind, g_inds] = 1\n",
    "    gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=mask_list[i])\n",
    "    \n",
    "    gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n",
    "\n",
    "pathways_genes = np.zeros((len(pathway_union), len(union_gene_miRNA))) \n",
    "for p  in pathway_union:\n",
    "    gs = paways_mirna_dicts[p]\n",
    "    g_inds = [union_gene_miRNA.index(g) for g in gs if g in union_gene_miRNA]\n",
    "    p_ind = pathway_union.index(p)\n",
    "    pathways_genes[p_ind, g_inds] = 1\n",
    "gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=union_gene_miRNA)\n",
    "\n",
    "gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a9d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gene_pathway_bp_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15821250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_pathway_bp_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125e5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import glorot_uniform, Initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D,Layer\n",
    "from tensorflow.keras import initializers,activations,regularizers\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    " \n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biological_module(Layer):\n",
    "    def __init__(self, units, mapp=None, nonzero_ind=None, kernel_initializer='glorot_uniform', W_regularizer=None,\n",
    "                 activation='tanh', use_bias=True,bias_initializer='zeros', bias_regularizer=None,\n",
    "                 bias_constraint=None,**kwargs):\n",
    "        \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.mapp = mapp\n",
    "        self.nonzero_ind = nonzero_ind\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activation_fn = activations.get(activation)\n",
    "        super(Biological_module, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        input_dim = input_shape[1]\n",
    "   \n",
    "\n",
    "        if not self.mapp is None:\n",
    "            self.mapp = self.mapp.astype(np.float32)\n",
    "\n",
    "   \n",
    "        if self.nonzero_ind is None:\n",
    "            nonzero_ind = np.array(np.nonzero(self.mapp)).T\n",
    "            self.nonzero_ind = nonzero_ind\n",
    "\n",
    "        self.kernel_shape = (input_dim, self.units)\n",
    "        \n",
    "\n",
    "        nonzero_count = self.nonzero_ind.shape[0]   \n",
    "\n",
    "\n",
    "        self.kernel_vector = self.add_weight(name='kernel_vector',\n",
    "                                             shape=(nonzero_count,),\n",
    "                                             initializer=self.kernel_initializer,\n",
    "                                             regularizer=self.kernel_regularizer,\n",
    "                                             trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer\n",
    "                                        )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        super(Biological_module, self).build(input_shape)  \n",
    "      \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        \n",
    "        trans = tf.scatter_nd(tf.constant(self.nonzero_ind, tf.int32), self.kernel_vector,\n",
    "                           tf.constant(list(self.kernel_shape)))\n",
    "    \n",
    "        output = K.dot(inputs, trans)\n",
    "        \n",
    "    \n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "            \n",
    "        if self.activation_fn is not None:\n",
    "            output = self.activation_fn(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': self.activation,\n",
    "            'use_bias': self.use_bias,\n",
    "            'nonzero_ind': np.array(self.nonzero_ind),\n",
    "          \n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'W_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "\n",
    "        }\n",
    "        base_config = super(Biological_module, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "      \n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c438cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    " \n",
    "    def __init__(self, output_dim,  W_regularizer=None,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      trainable=True)\n",
    " \n",
    "        super(Self_Attention, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    " \n",
    "\n",
    "        print(\"K.permute_dimensions(WK.shape\",(K.permute_dimensions(WK,[1,0]).shape))\n",
    " \n",
    "        QK =  K.dot(K.permute_dimensions(WK,[1,0]),WQ)\n",
    "    \n",
    " \n",
    "        QK = QK / (64**0.5)\n",
    " \n",
    "        QK = K.softmax(QK)\n",
    " \n",
    "        print(\"QK.shape\",QK.shape)\n",
    " \n",
    "        V = K.dot(WV,QK)\n",
    "        \n",
    "        print(V.shape)\n",
    " \n",
    "        return V\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          \n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "\n",
    "        }\n",
    "        base_config = super(Self_Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    " \n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab00da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020e30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mRNA_data,miRNA_data):\n",
    "    \n",
    "\n",
    "    \n",
    "    S_inputs_mRNA = Input(shape=(mRNA_data.shape[1],), dtype='float32',name= 'mRNA_inputs')\n",
    "  \n",
    "    S_inputs_miRNA = Input(shape=(miRNA_data.shape[1],), dtype='float32',name= 'miRNA_inputs')\n",
    "    \n",
    "\n",
    "   \n",
    "    h0_mRNA = Biological_module(gene_pathway_bp_dfs[0].shape[0],mapp =gene_pathway_bp_dfs[0].values.T, name = 'h0_mRNA',W_regularizer=l2(0.001))(S_inputs_mRNA)\n",
    "\n",
    "    \n",
    "    h0_miRNA = Biological_module(gene_pathway_bp_dfs[1].shape[0],mapp =gene_pathway_bp_dfs[1].values.T, name = 'h0_miRNA',W_regularizer=l2(0.001))(S_inputs_miRNA)\n",
    "\n",
    "\n",
    "\n",
    "    atten1 = Self_Attention(64,W_regularizer=l2(0.003))(h0_mRNA)\n",
    "    atten2 = Self_Attention(64,W_regularizer=l2(0.003))(h0_miRNA)\n",
    "    \n",
    "    feature_tal = tf.keras.layers.concatenate([atten1,atten2])\n",
    "\n",
    "    \n",
    "    h4 = tf.keras.layers.Dense(32,activation='tanh')(feature_tal)\n",
    "    \n",
    "    h5 = tf.keras.layers.Dense(1,activation='sigmoid')(h4)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[S_inputs_mRNA,S_inputs_miRNA], outputs=h5)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.0001,decay=0.0001) \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8181c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1044dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "   \n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def get_metrics(true_score,pre_score,pre_probe):\n",
    "    \n",
    "  \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_score, pre_probe, pos_label=1)\n",
    "   \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    aupr = average_precision_score(true_score, pre_probe)\n",
    "    \n",
    "    pre, rec, thresholds = precision_recall_curve(true_score, pre_probe)    \n",
    "    auprc  = metrics.auc(rec, pre)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(true_score,pre_score)\n",
    "    \n",
    "    f1 = metrics.f1_score(true_score, pre_score)\n",
    "    \n",
    "    precision = metrics.precision_score(true_score,pre_score)\n",
    "    \n",
    "    recall = metrics.recall_score(true_score,pre_score)\n",
    "    \n",
    "     \n",
    "    print( print(confusion_matrix(true_score,pre_score)))\n",
    "    return precision,accuracy,recall,f1,auc,aupr,auprc\n",
    "\n",
    "\n",
    "def evaluates(y_test, y_pred):\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    \n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)    \n",
    "    auprc  = metrics.auc(recall, precision)\n",
    "    \n",
    "    pp = [1 if index>=0.5  else 0 for index in  y_pred ]\n",
    "    \n",
    "    pre = metrics.precision_score(y_test,pp)\n",
    "    \n",
    "    f1 = metrics.f1_score(y_test,pp)\n",
    "    \n",
    "    rec = metrics.recall_score(y_test,pp)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test,pp)\n",
    "    \n",
    "    print(confusion_matrix(y_test,pp))\n",
    "    return pre,acc,rec,f1,auc,aupr,auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cad9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 120 101\n",
      "1.0940594059405941 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "y = example_case['response'].values\n",
    "n_samples =example_case['response'].values\n",
    "\n",
    "print(len(n_samples),n_samples.sum(),(len(n_samples) -n_samples.sum()))\n",
    "\n",
    "x_0 =  len(n_samples) / (2*  (len(n_samples) -n_samples.sum()))\n",
    "x_1 =  len(n_samples) / (2*  n_samples.sum())\n",
    "\n",
    "print(x_0,x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80621e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\10263\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention (Self_Attention (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_1 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           self__attention[0][0]            \n",
      "                                                                 self__attention_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           4128        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 176 samples\n",
      "Epoch 1/130\n",
      "176/176 [==============================] - 0s 700us/sample - loss: 0.9208 - acc: 0.4659\n",
      "Epoch 2/130\n",
      "176/176 [==============================] - 0s 95us/sample - loss: 0.9166 - acc: 0.4659\n",
      "Epoch 3/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.9125 - acc: 0.5966\n",
      "Epoch 4/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.9084 - acc: 0.6932\n",
      "Epoch 5/130\n",
      "176/176 [==============================] - 0s 87us/sample - loss: 0.9043 - acc: 0.6989\n",
      "Epoch 6/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.9002 - acc: 0.7102\n",
      "Epoch 7/130\n",
      "176/176 [==============================] - 0s 88us/sample - loss: 0.8962 - acc: 0.7273\n",
      "Epoch 8/130\n",
      "176/176 [==============================] - 0s 97us/sample - loss: 0.8922 - acc: 0.7159\n",
      "Epoch 9/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.8882 - acc: 0.7216\n",
      "Epoch 10/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8842 - acc: 0.7273\n",
      "Epoch 11/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.8802 - acc: 0.7330\n",
      "Epoch 12/130\n",
      "176/176 [==============================] - 0s 94us/sample - loss: 0.8761 - acc: 0.7386\n",
      "Epoch 13/130\n",
      "176/176 [==============================] - 0s 84us/sample - loss: 0.8720 - acc: 0.7386\n",
      "Epoch 14/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.8679 - acc: 0.7216\n",
      "Epoch 15/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.8637 - acc: 0.7159\n",
      "Epoch 16/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.8596 - acc: 0.7159\n",
      "Epoch 17/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8552 - acc: 0.7102\n",
      "Epoch 18/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.8506 - acc: 0.7102\n",
      "Epoch 19/130\n",
      "176/176 [==============================] - 0s 88us/sample - loss: 0.8461 - acc: 0.7102\n",
      "Epoch 20/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.8413 - acc: 0.7102\n",
      "Epoch 21/130\n",
      "176/176 [==============================] - 0s 89us/sample - loss: 0.8361 - acc: 0.7216\n",
      "Epoch 22/130\n",
      "176/176 [==============================] - 0s 88us/sample - loss: 0.8308 - acc: 0.7273\n",
      "Epoch 23/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.8253 - acc: 0.7330\n",
      "Epoch 24/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.8194 - acc: 0.7273\n",
      "Epoch 25/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.8125 - acc: 0.7273\n",
      "Epoch 26/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.8060 - acc: 0.7330\n",
      "Epoch 27/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.7987 - acc: 0.7330\n",
      "Epoch 28/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.7910 - acc: 0.7386\n",
      "Epoch 29/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.7826 - acc: 0.7386\n",
      "Epoch 30/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7740 - acc: 0.7557\n",
      "Epoch 31/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.7647 - acc: 0.7557\n",
      "Epoch 32/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.7553 - acc: 0.7500\n",
      "Epoch 33/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.7456 - acc: 0.7500\n",
      "Epoch 34/130\n",
      "176/176 [==============================] - 0s 72us/sample - loss: 0.7354 - acc: 0.7386\n",
      "Epoch 35/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.7260 - acc: 0.7386\n",
      "Epoch 36/130\n",
      "176/176 [==============================] - 0s 126us/sample - loss: 0.7152 - acc: 0.7443\n",
      "Epoch 37/130\n",
      "176/176 [==============================] - 0s 114us/sample - loss: 0.7055 - acc: 0.7386\n",
      "Epoch 38/130\n",
      "176/176 [==============================] - 0s 94us/sample - loss: 0.6959 - acc: 0.7386\n",
      "Epoch 39/130\n",
      "176/176 [==============================] - 0s 103us/sample - loss: 0.6865 - acc: 0.7386\n",
      "Epoch 40/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.6772 - acc: 0.7500\n",
      "Epoch 41/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.6686 - acc: 0.7500\n",
      "Epoch 42/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.6602 - acc: 0.7557\n",
      "Epoch 43/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.6527 - acc: 0.7557\n",
      "Epoch 44/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.6457 - acc: 0.7557\n",
      "Epoch 45/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.6377 - acc: 0.7670\n",
      "Epoch 46/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.6312 - acc: 0.7500\n",
      "Epoch 47/130\n",
      "176/176 [==============================] - 0s 69us/sample - loss: 0.6256 - acc: 0.7500\n",
      "Epoch 48/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.6205 - acc: 0.7443\n",
      "Epoch 49/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.6143 - acc: 0.7500\n",
      "Epoch 50/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.6086 - acc: 0.7614\n",
      "Epoch 51/130\n",
      "176/176 [==============================] - 0s 93us/sample - loss: 0.6032 - acc: 0.7727\n",
      "Epoch 52/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.5983 - acc: 0.7727\n",
      "Epoch 53/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.5933 - acc: 0.7670\n",
      "Epoch 54/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.5886 - acc: 0.7614\n",
      "Epoch 55/130\n",
      "176/176 [==============================] - 0s 72us/sample - loss: 0.5838 - acc: 0.7670\n",
      "Epoch 56/130\n",
      "176/176 [==============================] - 0s 84us/sample - loss: 0.5787 - acc: 0.7670\n",
      "Epoch 57/130\n",
      "176/176 [==============================] - 0s 93us/sample - loss: 0.5746 - acc: 0.7670\n",
      "Epoch 58/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.5693 - acc: 0.7727\n",
      "Epoch 59/130\n",
      "176/176 [==============================] - 0s 87us/sample - loss: 0.5656 - acc: 0.7727\n",
      "Epoch 60/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.5603 - acc: 0.7727\n",
      "Epoch 61/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.5561 - acc: 0.7727\n",
      "Epoch 62/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.5513 - acc: 0.7784\n",
      "Epoch 63/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.5474 - acc: 0.7784\n",
      "Epoch 64/130\n",
      "176/176 [==============================] - 0s 72us/sample - loss: 0.5429 - acc: 0.7784\n",
      "Epoch 65/130\n",
      "176/176 [==============================] - 0s 72us/sample - loss: 0.5388 - acc: 0.7784\n",
      "Epoch 66/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5352 - acc: 0.7784\n",
      "Epoch 67/130\n",
      "176/176 [==============================] - 0s 71us/sample - loss: 0.5308 - acc: 0.7727\n",
      "Epoch 68/130\n",
      "176/176 [==============================] - 0s 70us/sample - loss: 0.5268 - acc: 0.7841\n",
      "Epoch 69/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.5223 - acc: 0.7841\n",
      "Epoch 70/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.5182 - acc: 0.7898\n",
      "Epoch 71/130\n",
      "176/176 [==============================] - 0s 91us/sample - loss: 0.5146 - acc: 0.7898\n",
      "Epoch 72/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.5107 - acc: 0.8011\n",
      "Epoch 73/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.5069 - acc: 0.8011\n",
      "Epoch 74/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.5033 - acc: 0.7955\n",
      "Epoch 75/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.4992 - acc: 0.8011\n",
      "Epoch 76/130\n",
      "176/176 [==============================] - 0s 68us/sample - loss: 0.4956 - acc: 0.8011\n",
      "Epoch 77/130\n",
      "176/176 [==============================] - 0s 71us/sample - loss: 0.4929 - acc: 0.8068\n",
      "Epoch 78/130\n",
      "176/176 [==============================] - 0s 72us/sample - loss: 0.4884 - acc: 0.8011\n",
      "Epoch 79/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.4845 - acc: 0.8068\n",
      "Epoch 80/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.4810 - acc: 0.8068\n",
      "Epoch 81/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.4779 - acc: 0.8125\n",
      "Epoch 82/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.4739 - acc: 0.8125\n",
      "Epoch 83/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.4713 - acc: 0.8125\n",
      "Epoch 84/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4681 - acc: 0.8125\n",
      "Epoch 85/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.4636 - acc: 0.8068\n",
      "Epoch 86/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.4604 - acc: 0.8125\n",
      "Epoch 87/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.4570 - acc: 0.8239\n",
      "Epoch 88/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.4535 - acc: 0.8239\n",
      "Epoch 89/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.4503 - acc: 0.8239\n",
      "Epoch 90/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.4478 - acc: 0.8239\n",
      "Epoch 91/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.4439 - acc: 0.8239\n",
      "Epoch 92/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.4407 - acc: 0.8239\n",
      "Epoch 93/130\n",
      "176/176 [==============================] - 0s 75us/sample - loss: 0.4378 - acc: 0.8295\n",
      "Epoch 94/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.4345 - acc: 0.8295\n",
      "Epoch 95/130\n",
      "176/176 [==============================] - 0s 101us/sample - loss: 0.4312 - acc: 0.8295\n",
      "Epoch 96/130\n",
      "176/176 [==============================] - 0s 101us/sample - loss: 0.4283 - acc: 0.8295\n",
      "Epoch 97/130\n",
      "176/176 [==============================] - 0s 93us/sample - loss: 0.4254 - acc: 0.8295\n",
      "Epoch 98/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.4221 - acc: 0.8295\n",
      "Epoch 99/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.4201 - acc: 0.8295\n",
      "Epoch 100/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.4163 - acc: 0.8295\n",
      "Epoch 101/130\n",
      "176/176 [==============================] - 0s 82us/sample - loss: 0.4129 - acc: 0.8295\n",
      "Epoch 102/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.4103 - acc: 0.8352\n",
      "Epoch 103/130\n",
      "176/176 [==============================] - 0s 79us/sample - loss: 0.4069 - acc: 0.8352\n",
      "Epoch 104/130\n",
      "176/176 [==============================] - 0s 71us/sample - loss: 0.4047 - acc: 0.8295\n",
      "Epoch 105/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.4015 - acc: 0.8295\n",
      "Epoch 106/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.3985 - acc: 0.8352\n",
      "Epoch 107/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.3950 - acc: 0.8466\n",
      "Epoch 108/130\n",
      "176/176 [==============================] - 0s 71us/sample - loss: 0.3924 - acc: 0.8466\n",
      "Epoch 109/130\n",
      "176/176 [==============================] - 0s 87us/sample - loss: 0.3899 - acc: 0.8466\n",
      "Epoch 110/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3867 - acc: 0.8466\n",
      "Epoch 111/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.3837 - acc: 0.8466\n",
      "Epoch 112/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.3815 - acc: 0.8466\n",
      "Epoch 113/130\n",
      "176/176 [==============================] - 0s 74us/sample - loss: 0.3779 - acc: 0.8580\n",
      "Epoch 114/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.3752 - acc: 0.8580\n",
      "Epoch 115/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.3734 - acc: 0.8523\n",
      "Epoch 116/130\n",
      "176/176 [==============================] - 0s 70us/sample - loss: 0.3704 - acc: 0.8636\n",
      "Epoch 117/130\n",
      "176/176 [==============================] - 0s 72us/sample - loss: 0.3661 - acc: 0.8750\n",
      "Epoch 118/130\n",
      "176/176 [==============================] - 0s 80us/sample - loss: 0.3654 - acc: 0.8523\n",
      "Epoch 119/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.3627 - acc: 0.8523\n",
      "Epoch 120/130\n",
      "176/176 [==============================] - 0s 78us/sample - loss: 0.3601 - acc: 0.8580\n",
      "Epoch 121/130\n",
      "176/176 [==============================] - 0s 83us/sample - loss: 0.3560 - acc: 0.8750\n",
      "Epoch 122/130\n",
      "176/176 [==============================] - 0s 84us/sample - loss: 0.3538 - acc: 0.8807\n",
      "Epoch 123/130\n",
      "176/176 [==============================] - 0s 105us/sample - loss: 0.3512 - acc: 0.8864\n",
      "Epoch 124/130\n",
      "176/176 [==============================] - 0s 76us/sample - loss: 0.3491 - acc: 0.8807\n",
      "Epoch 125/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.3458 - acc: 0.8807\n",
      "Epoch 126/130\n",
      "176/176 [==============================] - 0s 85us/sample - loss: 0.3430 - acc: 0.8864\n",
      "Epoch 127/130\n",
      "176/176 [==============================] - 0s 77us/sample - loss: 0.3407 - acc: 0.8864\n",
      "Epoch 128/130\n",
      "176/176 [==============================] - 0s 90us/sample - loss: 0.3382 - acc: 0.8864\n",
      "Epoch 129/130\n",
      "176/176 [==============================] - 0s 81us/sample - loss: 0.3349 - acc: 0.8920\n",
      "Epoch 130/130\n",
      "176/176 [==============================] - 0s 86us/sample - loss: 0.3327 - acc: 0.8920\n",
      "[[17  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8492063492063492, 0.8948153750403589, 0.8927666255628246)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_2 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_3 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           self__attention_2[0][0]          \n",
      "                                                                 self__attention_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 570us/sample - loss: 0.9218 - acc: 0.4407\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.9175 - acc: 0.5311\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.9133 - acc: 0.6554\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.9090 - acc: 0.6949\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.9048 - acc: 0.7119\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9007 - acc: 0.7119\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8965 - acc: 0.7119\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 131us/sample - loss: 0.8924 - acc: 0.7119\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.8883 - acc: 0.7401\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8841 - acc: 0.7401\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.8801 - acc: 0.7514\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.8759 - acc: 0.7401\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8718 - acc: 0.7345\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.8676 - acc: 0.7345\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8633 - acc: 0.7288\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8589 - acc: 0.7232\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8544 - acc: 0.7232\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8498 - acc: 0.7288\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.8450 - acc: 0.7175\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8400 - acc: 0.7232\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8349 - acc: 0.7288\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.8292 - acc: 0.7345\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8236 - acc: 0.7401\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8174 - acc: 0.7458\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8107 - acc: 0.7345\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.8043 - acc: 0.7401\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7967 - acc: 0.7401\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.7889 - acc: 0.7514\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.7806 - acc: 0.7514\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.7722 - acc: 0.7514\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7630 - acc: 0.7514\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.7554 - acc: 0.7627\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.7454 - acc: 0.7627\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.7364 - acc: 0.7627\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7264 - acc: 0.7627\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 65us/sample - loss: 0.7181 - acc: 0.7458\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7092 - acc: 0.7458\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.6999 - acc: 0.7401\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6907 - acc: 0.7458\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.6813 - acc: 0.7458\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6723 - acc: 0.7740\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6644 - acc: 0.7740\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.6556 - acc: 0.7740\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.6480 - acc: 0.7684\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.6400 - acc: 0.7684\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.6328 - acc: 0.7684\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.6255 - acc: 0.7740\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6191 - acc: 0.7797\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.6119 - acc: 0.7797\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.6060 - acc: 0.7797\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.5999 - acc: 0.7797\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5938 - acc: 0.7910\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5874 - acc: 0.7910\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.5815 - acc: 0.7853\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.5757 - acc: 0.7910\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5705 - acc: 0.7853\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.5648 - acc: 0.7910\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5593 - acc: 0.7966\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.5542 - acc: 0.7966\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5485 - acc: 0.8079\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5444 - acc: 0.8023\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.5391 - acc: 0.8023\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.5340 - acc: 0.8079\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.5285 - acc: 0.8136\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5236 - acc: 0.8192\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.5190 - acc: 0.8192\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.5140 - acc: 0.8136\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 103us/sample - loss: 0.5095 - acc: 0.8192\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5052 - acc: 0.8249\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5001 - acc: 0.8249\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4965 - acc: 0.8249\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4916 - acc: 0.8305\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4871 - acc: 0.8305\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.4850 - acc: 0.8305\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.4788 - acc: 0.8418\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.4749 - acc: 0.8362\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.4716 - acc: 0.8305\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 125us/sample - loss: 0.4674 - acc: 0.8418\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 106us/sample - loss: 0.4633 - acc: 0.8418\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 104us/sample - loss: 0.4597 - acc: 0.8475\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.4564 - acc: 0.8475\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.4525 - acc: 0.8475\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.4489 - acc: 0.8588\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4452 - acc: 0.8531\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4416 - acc: 0.8531\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4380 - acc: 0.8531\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.4349 - acc: 0.8588\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4315 - acc: 0.8588\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.4282 - acc: 0.8588\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.4248 - acc: 0.8588\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4215 - acc: 0.8588\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4182 - acc: 0.8588\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4159 - acc: 0.8588\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.4118 - acc: 0.8588\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4091 - acc: 0.8644\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.4066 - acc: 0.8644\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4026 - acc: 0.8701\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.3996 - acc: 0.8644\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.3967 - acc: 0.8644\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3936 - acc: 0.8644\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3910 - acc: 0.8701\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3890 - acc: 0.8701\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3849 - acc: 0.8757\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.3820 - acc: 0.8814\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.3795 - acc: 0.8757\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.3766 - acc: 0.8814\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3734 - acc: 0.8870\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 66us/sample - loss: 0.3710 - acc: 0.8927\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.3687 - acc: 0.8814\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3657 - acc: 0.8870\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3630 - acc: 0.8983\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.3602 - acc: 0.8927\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.3570 - acc: 0.8983\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.3541 - acc: 0.8983\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3517 - acc: 0.8983\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3491 - acc: 0.8983\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3465 - acc: 0.8983\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3437 - acc: 0.8983\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3416 - acc: 0.8983\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.3390 - acc: 0.8927\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3361 - acc: 0.8927\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3333 - acc: 0.8983\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3311 - acc: 0.9040\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3287 - acc: 0.9040\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3270 - acc: 0.8983\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3232 - acc: 0.8983\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3208 - acc: 0.9040\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.3191 - acc: 0.9153\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 72us/sample - loss: 0.3159 - acc: 0.9153\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3137 - acc: 0.8983\n",
      "[[14  6]\n",
      " [ 7 17]]\n",
      "None\n",
      "(0.7391304347826086, 0.7045454545454546, 0.7083333333333334, 0.723404255319149, 0.78125, 0.8141424463121578, 0.8084833717506148)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_4 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_5 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           self__attention_4[0][0]          \n",
      "                                                                 self__attention_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           4128        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            33          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 635us/sample - loss: 0.9220 - acc: 0.5480\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9178 - acc: 0.5763\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.9137 - acc: 0.5876\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.9095 - acc: 0.5819\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.9054 - acc: 0.6215\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.9014 - acc: 0.6610\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.8973 - acc: 0.6610\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.8932 - acc: 0.6667\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.8892 - acc: 0.6554\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8851 - acc: 0.6554\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.8811 - acc: 0.6610\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.8770 - acc: 0.6610\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.8729 - acc: 0.6610\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 66us/sample - loss: 0.8687 - acc: 0.6610\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.8644 - acc: 0.6554\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8601 - acc: 0.6554\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.8576 - acc: 0.703 - 0s 73us/sample - loss: 0.8556 - acc: 0.6554\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.8510 - acc: 0.6610\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.8462 - acc: 0.6723\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.8414 - acc: 0.6780\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8362 - acc: 0.6893\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.8309 - acc: 0.7006\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.8254 - acc: 0.7119\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.8203 - acc: 0.734 - 0s 70us/sample - loss: 0.8193 - acc: 0.7062\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 72us/sample - loss: 0.8134 - acc: 0.7062\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 72us/sample - loss: 0.8065 - acc: 0.7175\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.7997 - acc: 0.7232\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.7925 - acc: 0.6949\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.7840 - acc: 0.7119\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7760 - acc: 0.7175\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.7672 - acc: 0.7175\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.7584 - acc: 0.7288\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 72us/sample - loss: 0.7499 - acc: 0.7288\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.7407 - acc: 0.7232\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7316 - acc: 0.7232\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.7224 - acc: 0.7232\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.7139 - acc: 0.7232\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.7047 - acc: 0.7232\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.6963 - acc: 0.7288\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.6881 - acc: 0.7232\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.6806 - acc: 0.7232\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.6733 - acc: 0.7288\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.6661 - acc: 0.7288\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6592 - acc: 0.7288\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.6530 - acc: 0.7345\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.6476 - acc: 0.7288\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.6405 - acc: 0.7288\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 101us/sample - loss: 0.6359 - acc: 0.7288\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 126us/sample - loss: 0.6301 - acc: 0.7345\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6255 - acc: 0.7345\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.6206 - acc: 0.7401\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6155 - acc: 0.7401\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.6106 - acc: 0.7401\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.6053 - acc: 0.7514\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.6008 - acc: 0.7514\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.5967 - acc: 0.7514\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5918 - acc: 0.7627\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5875 - acc: 0.7571\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 62us/sample - loss: 0.5827 - acc: 0.7458\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.5787 - acc: 0.7458\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5744 - acc: 0.7514\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5692 - acc: 0.7571\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5651 - acc: 0.7627\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.5620 - acc: 0.7571\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.5570 - acc: 0.7627\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5524 - acc: 0.7797\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5488 - acc: 0.7910\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.5447 - acc: 0.7797\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5402 - acc: 0.7853\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.5358 - acc: 0.7853\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.5317 - acc: 0.7910\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5285 - acc: 0.7910\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5239 - acc: 0.7910\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.5201 - acc: 0.7910\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.5158 - acc: 0.8023\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5119 - acc: 0.7966\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.5087 - acc: 0.7966\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 66us/sample - loss: 0.5041 - acc: 0.7966\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.5006 - acc: 0.8023\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.4966 - acc: 0.8079\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4923 - acc: 0.8079\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4887 - acc: 0.8192\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.4846 - acc: 0.8136\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4809 - acc: 0.8192\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4777 - acc: 0.8249\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4734 - acc: 0.8249\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4698 - acc: 0.8305\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.4663 - acc: 0.8305\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.4627 - acc: 0.8305\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.4588 - acc: 0.8305\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 65us/sample - loss: 0.4558 - acc: 0.8362\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4511 - acc: 0.8362\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.4481 - acc: 0.8418\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 110us/sample - loss: 0.4460 - acc: 0.8418\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 118us/sample - loss: 0.4419 - acc: 0.8418\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 111us/sample - loss: 0.4376 - acc: 0.8418\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 106us/sample - loss: 0.4338 - acc: 0.8362\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4314 - acc: 0.8475\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 72us/sample - loss: 0.4280 - acc: 0.8531\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.4247 - acc: 0.8531\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4207 - acc: 0.8531\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4176 - acc: 0.8418\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4143 - acc: 0.8475\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4116 - acc: 0.8531\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4080 - acc: 0.8531\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4055 - acc: 0.8531\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4016 - acc: 0.8531\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3981 - acc: 0.8701\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3960 - acc: 0.8757\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.3927 - acc: 0.8701\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3885 - acc: 0.8757\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3857 - acc: 0.8701\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3832 - acc: 0.8701\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3806 - acc: 0.8814\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3767 - acc: 0.8814\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3744 - acc: 0.8757\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3706 - acc: 0.8870\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.3678 - acc: 0.8870\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3660 - acc: 0.8927\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3617 - acc: 0.8870\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 68us/sample - loss: 0.3592 - acc: 0.8927\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.3559 - acc: 0.8983\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3536 - acc: 0.8983\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3501 - acc: 0.8927\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3479 - acc: 0.8927\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3456 - acc: 0.8927\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3419 - acc: 0.8927\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 70us/sample - loss: 0.3386 - acc: 0.8983\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.3364 - acc: 0.8983\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.3346 - acc: 0.8983\n",
      "[[16  4]\n",
      " [ 5 19]]\n",
      "None\n",
      "(0.8260869565217391, 0.7954545454545454, 0.7916666666666666, 0.8085106382978724, 0.89375, 0.9105907153796968, 0.9083594882811489)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_6 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_7 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           self__attention_6[0][0]          \n",
      "                                                                 self__attention_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           4128        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 688us/sample - loss: 0.9230 - acc: 0.5367\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.9188 - acc: 0.5424\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.9147 - acc: 0.5424\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 104us/sample - loss: 0.9105 - acc: 0.5706\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.9064 - acc: 0.6102\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.9024 - acc: 0.6045\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.8983 - acc: 0.6271\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 138us/sample - loss: 0.8943 - acc: 0.6497\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 122us/sample - loss: 0.8903 - acc: 0.6667\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.8863 - acc: 0.6667\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.8823 - acc: 0.6723\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8784 - acc: 0.6723\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.8744 - acc: 0.6667\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8704 - acc: 0.6667\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.8664 - acc: 0.6554\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.8624 - acc: 0.6667\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8584 - acc: 0.6667\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8543 - acc: 0.6723\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8500 - acc: 0.6836\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.8458 - acc: 0.6780\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.8412 - acc: 0.6949\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.8366 - acc: 0.6893\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.8320 - acc: 0.7006\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.8271 - acc: 0.7006\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.8222 - acc: 0.7006\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.8169 - acc: 0.7062\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.8114 - acc: 0.7119\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.8056 - acc: 0.7232\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7994 - acc: 0.7175\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.7928 - acc: 0.7175\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7860 - acc: 0.7062\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.7785 - acc: 0.7175\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.7711 - acc: 0.7175\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.7636 - acc: 0.7062\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.7558 - acc: 0.7119\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.7472 - acc: 0.7119\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7394 - acc: 0.7119\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.7308 - acc: 0.7119\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.7226 - acc: 0.7175\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 81us/sample - loss: 0.7140 - acc: 0.7175\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.7058 - acc: 0.7345\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.6980 - acc: 0.7345\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.6901 - acc: 0.7345\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6823 - acc: 0.7288\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.6750 - acc: 0.7175\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.6672 - acc: 0.7345\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.6609 - acc: 0.7288\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 139us/sample - loss: 0.6534 - acc: 0.7345\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.6471 - acc: 0.7458\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.6410 - acc: 0.7458\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.6343 - acc: 0.7571\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 102us/sample - loss: 0.6281 - acc: 0.7571\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6226 - acc: 0.7458\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6166 - acc: 0.7514\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.6107 - acc: 0.7571\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 72us/sample - loss: 0.6049 - acc: 0.7571\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5993 - acc: 0.7627\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5942 - acc: 0.7740\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5883 - acc: 0.7740\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.5842 - acc: 0.7740\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5785 - acc: 0.7740\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5726 - acc: 0.7627\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5676 - acc: 0.7684\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5626 - acc: 0.7740\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5572 - acc: 0.7740\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.5530 - acc: 0.7740\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.5475 - acc: 0.7797\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5438 - acc: 0.7853\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.5383 - acc: 0.7966\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5330 - acc: 0.8023\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.5288 - acc: 0.8023\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5239 - acc: 0.8023\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.5204 - acc: 0.7966\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.5148 - acc: 0.8023\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.5100 - acc: 0.8079\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.5064 - acc: 0.8192\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5022 - acc: 0.8192\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4982 - acc: 0.8192\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4927 - acc: 0.8249\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.4894 - acc: 0.8249\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.4848 - acc: 0.8249\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 122us/sample - loss: 0.4819 - acc: 0.8305\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 116us/sample - loss: 0.4762 - acc: 0.8249\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.4761 - acc: 0.8362\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.4707 - acc: 0.8418\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.4659 - acc: 0.8249\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4619 - acc: 0.8362\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4581 - acc: 0.8418\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4547 - acc: 0.8475\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4513 - acc: 0.8475\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.4476 - acc: 0.8475\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.4438 - acc: 0.8475\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4407 - acc: 0.8475\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.4370 - acc: 0.8588\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4338 - acc: 0.8644\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.4307 - acc: 0.8701\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.4270 - acc: 0.8701\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4240 - acc: 0.8814\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4205 - acc: 0.8814\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4171 - acc: 0.8757\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4173 - acc: 0.8644\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.4102 - acc: 0.8757\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4073 - acc: 0.8814\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.4052 - acc: 0.8757\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.4022 - acc: 0.8814\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3985 - acc: 0.8870\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.3946 - acc: 0.8870\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.3933 - acc: 0.8814\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.3904 - acc: 0.8870\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.3861 - acc: 0.8870\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3845 - acc: 0.8814\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 103us/sample - loss: 0.3826 - acc: 0.8814\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.3778 - acc: 0.8814\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3750 - acc: 0.8870\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3722 - acc: 0.8870\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3688 - acc: 0.8870\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 138us/sample - loss: 0.3659 - acc: 0.8927\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.3634 - acc: 0.8927\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.3602 - acc: 0.8870\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.3573 - acc: 0.8870\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.3551 - acc: 0.8927\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.3534 - acc: 0.8983\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 67us/sample - loss: 0.3485 - acc: 0.8983\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.3488 - acc: 0.8927\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 75us/sample - loss: 0.3459 - acc: 0.8927\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.3416 - acc: 0.8983\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.3414 - acc: 0.9040\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3388 - acc: 0.9153\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3350 - acc: 0.9096\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 69us/sample - loss: 0.3333 - acc: 0.8983\n",
      "[[16  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8181818181818182, 0.8333333333333334, 0.8333333333333334, 0.9020833333333333, 0.9300974921699026, 0.9287232278936298)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mRNA_inputs (InputLayer)        [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "miRNA_inputs (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h0_mRNA (Biological_module)     (None, 238)          6087        mRNA_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "h0_miRNA (Biological_module)    (None, 238)          10267       miRNA_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_8 (Self_Attenti (None, 64)           45696       h0_mRNA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_9 (Self_Attenti (None, 64)           45696       h0_miRNA[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           self__attention_8[0][0]          \n",
      "                                                                 self__attention_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           4128        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            33          dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 650us/sample - loss: 0.9227 - acc: 0.5141\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.9183 - acc: 0.5989\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.9139 - acc: 0.6497\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 80us/sample - loss: 0.9096 - acc: 0.6723\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.9052 - acc: 0.6780\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 118us/sample - loss: 0.9009 - acc: 0.6723\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.8966 - acc: 0.6554\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.8922 - acc: 0.6610\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 100us/sample - loss: 0.8878 - acc: 0.6723\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8835 - acc: 0.6610\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8792 - acc: 0.6610\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8749 - acc: 0.6554\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8704 - acc: 0.6554\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.8659 - acc: 0.6667\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.8613 - acc: 0.6723\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8566 - acc: 0.6780\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.8517 - acc: 0.6893\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.8467 - acc: 0.6949\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.8414 - acc: 0.7062\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.8360 - acc: 0.7232\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.8299 - acc: 0.7345\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 130us/sample - loss: 0.8238 - acc: 0.7288\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 137us/sample - loss: 0.8170 - acc: 0.7232\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.8105 - acc: 0.7119\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 124us/sample - loss: 0.8029 - acc: 0.7232\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.7953 - acc: 0.7401\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 95us/sample - loss: 0.7874 - acc: 0.7401\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.7787 - acc: 0.7514\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.7700 - acc: 0.7458\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.7610 - acc: 0.7514\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.7512 - acc: 0.7514\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 115us/sample - loss: 0.7420 - acc: 0.7514\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.7325 - acc: 0.7514\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.7226 - acc: 0.7514\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.7134 - acc: 0.7514\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.7037 - acc: 0.7571\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.6942 - acc: 0.7514\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 97us/sample - loss: 0.6857 - acc: 0.7571\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.6764 - acc: 0.7627\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 94us/sample - loss: 0.6675 - acc: 0.7571\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.6596 - acc: 0.7627\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.6513 - acc: 0.7627\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.6443 - acc: 0.7571\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.6364 - acc: 0.7571\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6310 - acc: 0.7571\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.6239 - acc: 0.7571\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.6172 - acc: 0.7627\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.6113 - acc: 0.7684\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.6054 - acc: 0.7684\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.6005 - acc: 0.7684\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5935 - acc: 0.7684\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5887 - acc: 0.7797\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 129us/sample - loss: 0.5835 - acc: 0.7853\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 127us/sample - loss: 0.5778 - acc: 0.7853\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 129us/sample - loss: 0.5756 - acc: 0.7853\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 113us/sample - loss: 0.5682 - acc: 0.7853\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.5626 - acc: 0.7853\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.5575 - acc: 0.7910\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 73us/sample - loss: 0.5525 - acc: 0.7966\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.5476 - acc: 0.7966\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.5431 - acc: 0.8023\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.5384 - acc: 0.8023\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.5337 - acc: 0.8079\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5288 - acc: 0.8136\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 93us/sample - loss: 0.5245 - acc: 0.8136\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.5197 - acc: 0.8136\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.5168 - acc: 0.8192\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.5107 - acc: 0.8249\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 89us/sample - loss: 0.5060 - acc: 0.8249\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.5017 - acc: 0.8136\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.4976 - acc: 0.8136\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4929 - acc: 0.8192\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4883 - acc: 0.8249\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.4842 - acc: 0.8418\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.4798 - acc: 0.8362\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.4765 - acc: 0.8418\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.4711 - acc: 0.8531\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.4676 - acc: 0.8531\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4631 - acc: 0.8475\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.4589 - acc: 0.8531\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 91us/sample - loss: 0.4548 - acc: 0.8531\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 101us/sample - loss: 0.4516 - acc: 0.8588\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 112us/sample - loss: 0.4472 - acc: 0.8531\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 139us/sample - loss: 0.4433 - acc: 0.8531\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 123us/sample - loss: 0.4388 - acc: 0.8531\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4349 - acc: 0.8588\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 114us/sample - loss: 0.4316 - acc: 0.8588\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.4274 - acc: 0.8588\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 98us/sample - loss: 0.4234 - acc: 0.8588\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 105us/sample - loss: 0.4197 - acc: 0.8588\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 88us/sample - loss: 0.4167 - acc: 0.8644\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.4119 - acc: 0.8588\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.4091 - acc: 0.8701\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.4064 - acc: 0.8701\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 77us/sample - loss: 0.4012 - acc: 0.8757\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 74us/sample - loss: 0.3971 - acc: 0.8814\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3940 - acc: 0.8870\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3904 - acc: 0.8927\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 90us/sample - loss: 0.3865 - acc: 0.8927\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3834 - acc: 0.8927\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 92us/sample - loss: 0.3796 - acc: 0.8983\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 96us/sample - loss: 0.3761 - acc: 0.9040\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 115us/sample - loss: 0.3728 - acc: 0.9040\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.3695 - acc: 0.9040\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 108us/sample - loss: 0.3661 - acc: 0.9040\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 126us/sample - loss: 0.3623 - acc: 0.9096\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 86us/sample - loss: 0.3587 - acc: 0.9096\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3554 - acc: 0.9096\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 78us/sample - loss: 0.3521 - acc: 0.9096\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3489 - acc: 0.9153\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3452 - acc: 0.9153\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 107us/sample - loss: 0.3419 - acc: 0.9153\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 142us/sample - loss: 0.3387 - acc: 0.9209\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 134us/sample - loss: 0.3358 - acc: 0.9209\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 99us/sample - loss: 0.3318 - acc: 0.9209\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 82us/sample - loss: 0.3292 - acc: 0.9266\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3254 - acc: 0.9266\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.3222 - acc: 0.9322\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.3191 - acc: 0.9322\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 83us/sample - loss: 0.3156 - acc: 0.9322\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3126 - acc: 0.9379\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 76us/sample - loss: 0.3094 - acc: 0.9379\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3065 - acc: 0.9322\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 71us/sample - loss: 0.3040 - acc: 0.9379\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 79us/sample - loss: 0.3001 - acc: 0.9379\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.2969 - acc: 0.9379\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.2940 - acc: 0.9435\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 87us/sample - loss: 0.2913 - acc: 0.9435\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 85us/sample - loss: 0.2889 - acc: 0.9435\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 84us/sample - loss: 0.2858 - acc: 0.9435\n",
      "[[14  6]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.7692307692307693, 0.7727272727272727, 0.8333333333333334, 0.8, 0.8145833333333333, 0.8644454517588098, 0.8615687199798585)\n",
      "Cross validated results :  ACC = 0.7826262626262626, REC = 0.8, F1 = 0.7997163120567377, AUC = 0.8481746031746032, AUPR =0.8828182961321851\n"
     ]
    }
   ],
   "source": [
    "#Five-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1030) \n",
    "\n",
    "kfscore = []\n",
    "p = 0\n",
    "for train_index, test_index in skf.split(mRNA_data.values,y):\n",
    "\n",
    "\n",
    "    mRNA_train_x = mRNA_data.values[train_index]\n",
    "    mRNA_test_x  = mRNA_data.values[test_index]\n",
    "\n",
    "    miRNA_train_x = miRNA_data.values[train_index]\n",
    "    miRNA_test_x  = miRNA_data.values[test_index]\n",
    "\n",
    "    train_y  = y[train_index]\n",
    "    test_y   = y[test_index]\n",
    "\n",
    "    model = create_model(mRNA_data,miRNA_data)\n",
    "    model.fit( { \"mRNA_inputs\": mRNA_train_x, 'miRNA_inputs':miRNA_train_x},train_y,\n",
    "                 epochs=130,batch_size = 64,class_weight = {0:x_0,1:x_1})  \n",
    "\n",
    "    y_pred = model.predict({\"mRNA_inputs\": mRNA_test_x,'miRNA_inputs':miRNA_test_x})\n",
    "\n",
    "    y_score = [1 if index>=0.5  else 0 for index in  y_pred]\n",
    "\n",
    "    evaluate_epoch = get_metrics(test_y,y_score,y_pred)\n",
    "    print(evaluate_epoch)\n",
    "\n",
    "    kfscore.append(evaluate_epoch)\n",
    "    \n",
    "results = list(np.array(kfscore).sum(axis= 0)/5.0)\n",
    "print('Cross validated results :  ACC = {}, REC = {}, F1 = {}, AUC = {}, AUPR ={}'.format(results[1],results[2],results[3],results[4],results[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58a98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e535d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepkngg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
